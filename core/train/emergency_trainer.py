# PI_GAN_THZ/core/train/emergency_trainer.py
# ç´§æ€¥ä¿®å¤è®­ç»ƒå™¨ - é’ˆå¯¹å½“å‰è¯„ä¼°ç»“æœçš„é—®é¢˜

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
import numpy as np
import argparse
import time
import matplotlib.pyplot as plt
from typing import Dict, Tuple, Optional

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

from core.models.generator import Generator
from core.models.discriminator import Discriminator
from core.models.forward_model import ForwardModel
from core.utils.data_loader import MetamaterialDataset, denormalize_params, denormalize_metrics
from core.utils.set_seed import set_seed
from core.utils.loss import criterion_bce, criterion_mse
import config.config as cfg

class EmergencyTrainer:
    """
    ç´§æ€¥ä¿®å¤è®­ç»ƒå™¨ - é’ˆå¯¹å½“å‰é—®é¢˜çš„ä¸“é—¨è§£å†³æ–¹æ¡ˆ
    
    é’ˆå¯¹é—®é¢˜:
    1. å‰å‘ç½‘ç»œRÂ²=-0.1768 (æå·®)
    2. ç”Ÿæˆå™¨RÂ²=-0.3637 (æå·®) 
    3. å¾ªç¯ä¸€è‡´æ€§=0.2062 (ä¸¥é‡)
    4. åˆ¤åˆ«å™¨è¿‡å¼º=0.9225
    """
    
    def __init__(self, device: str = "auto"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu") if device == "auto" else torch.device(device)
        
        # æ¨¡å‹
        self.generator = None
        self.discriminator = None
        self.forward_model = None
        
        # ä¼˜åŒ–å™¨
        self.optimizer_G = None
        self.optimizer_D = None
        self.optimizer_F = None
        
        # è°ƒåº¦å™¨
        self.scheduler_G = None
        self.scheduler_D = None
        self.scheduler_F = None
        
        # æŸå¤±å‡½æ•°
        self.criterion_bce = criterion_bce()
        self.criterion_mse = criterion_mse()
        self.criterion_l1 = nn.L1Loss()
        
        # ç´§æ€¥ä¿®å¤é…ç½®
        self.emergency_config = {
            # å‰å‘ç½‘ç»œå¼ºåŒ–è®­ç»ƒ
            'forward_intensive_epochs': 200,  # å¤§å¹…å¢åŠ å‰å‘ç½‘ç»œè®­ç»ƒ
            'forward_lr': 5e-4,              # æé«˜å‰å‘ç½‘ç»œå­¦ä¹ ç‡
            
            # å¹³è¡¡åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨
            'discriminator_lr': 5e-5,         # å¤§å¹…é™ä½åˆ¤åˆ«å™¨å­¦ä¹ ç‡
            'generator_lr': 2e-4,             # ä¿æŒç”Ÿæˆå™¨å­¦ä¹ ç‡
            'discriminator_update_freq': 2,   # åˆ¤åˆ«å™¨æ¯2è½®æ›´æ–°ä¸€æ¬¡
            
            # æŸå¤±æƒé‡é‡æ–°å¹³è¡¡
            'forward_consistency_weight': 20.0,  # å¤§å¹…æé«˜ä¸€è‡´æ€§æƒé‡
            'reconstruction_weight': 15.0,       # æé«˜é‡å»ºæƒé‡
            'adversarial_weight': 0.1,          # å¤§å¹…é™ä½å¯¹æŠ—æƒé‡
            'l1_penalty_weight': 5.0,           # æ·»åŠ L1æ­£åˆ™åŒ–
            
            # æ¸è¿›å¼è®­ç»ƒ
            'warmup_epochs': 100,               # 100è½®warmup
            'progressive_adversarial': True,    # æ¸è¿›å¼å¯¹æŠ—è®­ç»ƒ
        }
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'forward_losses': [],
            'g_losses': [],
            'd_losses': [],
            'consistency_errors': [],
            'r2_scores': [],
            'lr_history': {'generator': [], 'discriminator': [], 'forward_model': []}
        }
        
        print(f"Emergency Trainer initialized on device: {self.device}")
        print("ğŸš¨ é’ˆå¯¹å½“å‰é—®é¢˜çš„ç´§æ€¥ä¿®å¤æ¨¡å¼")
        
    def initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print("Initializing models for emergency training...")
        
        self.generator = Generator(
            input_dim=cfg.SPECTRUM_DIM,
            output_dim=cfg.GENERATOR_OUTPUT_PARAM_DIM
        ).to(self.device)
        
        self.discriminator = Discriminator(
            input_spec_dim=cfg.DISCRIMINATOR_INPUT_SPEC_DIM,
            input_param_dim=cfg.DISCRIMINATOR_INPUT_PARAM_DIM
        ).to(self.device)
        
        self.forward_model = ForwardModel(
            input_param_dim=cfg.FORWARD_MODEL_INPUT_DIM,
            output_spectrum_dim=cfg.FORWARD_MODEL_OUTPUT_SPEC_DIM,
            output_metrics_dim=cfg.FORWARD_MODEL_OUTPUT_METRICS_DIM
        ).to(self.device)
        
        print("âœ“ Models initialized")
        
    def initialize_optimizers(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨ - ä½¿ç”¨ç´§æ€¥ä¿®å¤é…ç½®"""
        print("Initializing emergency optimizers...")
        
        # å‰å‘æ¨¡å‹ - æé«˜å­¦ä¹ ç‡
        self.optimizer_F = optim.Adam(
            self.forward_model.parameters(),
            lr=self.emergency_config['forward_lr'],
            betas=(0.9, 0.999),
            weight_decay=1e-4
        )
        self.scheduler_F = ReduceLROnPlateau(
            self.optimizer_F, mode='min', factor=0.5, patience=20, verbose=True
        )
        
        # ç”Ÿæˆå™¨ - ä¿æŒå­¦ä¹ ç‡
        self.optimizer_G = optim.Adam(
            self.generator.parameters(),
            lr=self.emergency_config['generator_lr'],
            betas=(0.5, 0.999),
            weight_decay=1e-4
        )
        self.scheduler_G = ReduceLROnPlateau(
            self.optimizer_G, mode='min', factor=0.7, patience=15, verbose=True
        )
        
        # åˆ¤åˆ«å™¨ - å¤§å¹…é™ä½å­¦ä¹ ç‡
        self.optimizer_D = optim.Adam(
            self.discriminator.parameters(),
            lr=self.emergency_config['discriminator_lr'],
            betas=(0.5, 0.999),
            weight_decay=1e-4
        )
        self.scheduler_D = ReduceLROnPlateau(
            self.optimizer_D, mode='min', factor=0.8, patience=25, verbose=True
        )
        
        print("âœ“ Emergency optimizers initialized")
        print(f"  - Forward Model LR: {self.emergency_config['forward_lr']}")
        print(f"  - Generator LR: {self.emergency_config['generator_lr']}")
        print(f"  - Discriminator LR: {self.emergency_config['discriminator_lr']} (å¤§å¹…é™ä½)")
        
    def intensive_forward_training(self, dataloader, num_epochs: int = 200):
        """
        å‰å‘ç½‘ç»œå¼ºåŒ–è®­ç»ƒ - è§£å†³RÂ²=-0.1768é—®é¢˜
        """
        print(f"\nğŸ”¥ INTENSIVE FORWARD MODEL TRAINING ({num_epochs} epochs)")
        print("ç›®æ ‡: è§£å†³å‰å‘ç½‘ç»œRÂ²=-0.1768çš„ä¸¥é‡é—®é¢˜")
        
        self.forward_model.train()
        best_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            spectrum_loss_sum = 0.0
            metrics_loss_sum = 0.0
            
            for batch_idx, batch in enumerate(dataloader):
                real_spectrum, _, real_params_norm, _, real_metrics_norm = batch
                
                real_spectrum = real_spectrum.to(self.device)
                real_params_norm = real_params_norm.to(self.device)
                real_metrics_norm = real_metrics_norm.to(self.device)
                
                # å‰å‘ä¼ æ’­
                pred_spectrum, pred_metrics = self.forward_model(real_params_norm)
                
                # å¤šé‡æŸå¤±
                spectrum_loss = self.criterion_mse(pred_spectrum, real_spectrum)
                metrics_loss = self.criterion_mse(pred_metrics, real_metrics_norm)
                
                # L1æ­£åˆ™åŒ–å¢å¼ºæ³›åŒ–èƒ½åŠ›
                spectrum_l1_loss = self.criterion_l1(pred_spectrum, real_spectrum)
                metrics_l1_loss = self.criterion_l1(pred_metrics, real_metrics_norm)
                
                # å¹³æ»‘æ€§æŸå¤±
                spectrum_diff = torch.diff(pred_spectrum, dim=1)
                smoothness_loss = torch.mean(spectrum_diff ** 2)
                
                # æ€»æŸå¤± - é‡æ–°å¹³è¡¡æƒé‡
                total_loss = (
                    5.0 * spectrum_loss +        # æé«˜å…‰è°±æƒé‡
                    3.0 * metrics_loss +         # æé«˜æŒ‡æ ‡æƒé‡
                    2.0 * spectrum_l1_loss +     # æ·»åŠ L1æ­£åˆ™
                    1.0 * metrics_l1_loss +      # æ·»åŠ L1æ­£åˆ™
                    0.5 * smoothness_loss        # å¹³æ»‘æ€§
                )
                
                # åå‘ä¼ æ’­
                self.optimizer_F.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.forward_model.parameters(), 0.5)
                self.optimizer_F.step()
                
                epoch_loss += total_loss.item()
                spectrum_loss_sum += spectrum_loss.item()
                metrics_loss_sum += metrics_loss.item()
            
            # å¹³å‡æŸå¤±
            avg_loss = epoch_loss / len(dataloader)
            avg_spectrum_loss = spectrum_loss_sum / len(dataloader)
            avg_metrics_loss = metrics_loss_sum / len(dataloader)
            
            # è®°å½•å†å²
            self.train_history['forward_losses'].append(avg_loss)
            self.train_history['lr_history']['forward_model'].append(
                self.optimizer_F.param_groups[0]['lr']
            )
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler_F.step(avg_loss)
            
            # æ—©åœæ£€æŸ¥
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³å‰å‘æ¨¡å‹
                torch.save(self.forward_model.state_dict(), 
                          os.path.join(cfg.SAVED_MODELS_DIR, "forward_model_best.pth"))
            else:
                patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{num_epochs}]")
                print(f"  Total Loss: {avg_loss:.6f}")
                print(f"  Spectrum Loss: {avg_spectrum_loss:.6f}")
                print(f"  Metrics Loss: {avg_metrics_loss:.6f}")
                print(f"  LR: {self.optimizer_F.param_groups[0]['lr']:.6f}")
                print(f"  Best Loss: {best_loss:.6f}")
            
            # æ—©åœ
            if patience_counter >= 30:
                print(f"Early stopping at epoch {epoch+1}")
                break
        
        print("âœ“ Intensive forward model training completed")
        print(f"  Final Loss: {avg_loss:.6f}")
        print(f"  Best Loss: {best_loss:.6f}")
        
    def balanced_gan_training(self, dataloader, dataset, num_epochs: int = 200):
        """
        å¹³è¡¡GANè®­ç»ƒ - è§£å†³åˆ¤åˆ«å™¨è¿‡å¼ºé—®é¢˜
        """
        print(f"\nâš–ï¸ BALANCED GAN TRAINING ({num_epochs} epochs)")
        print("ç›®æ ‡: å¹³è¡¡åˆ¤åˆ«å™¨(å½“å‰92.25%)å’Œç”Ÿæˆå™¨æ€§èƒ½")
        
        discriminator_update_counter = 0
        
        for epoch in range(num_epochs):
            epoch_losses = {
                'g_loss': 0.0,
                'd_loss': 0.0,
                'consistency_error': 0.0,
                'r2_score': 0.0
            }
            
            for batch_idx, batch in enumerate(dataloader):
                real_spectrum, real_params_denorm, real_params_norm, _, _ = batch
                
                real_spectrum = real_spectrum.to(self.device)
                real_params_denorm = real_params_denorm.to(self.device)
                real_params_norm = real_params_norm.to(self.device)
                
                batch_size = real_spectrum.size(0)
                
                # ===================
                # è®­ç»ƒç”Ÿæˆå™¨ (æ¯è½®éƒ½è®­ç»ƒ)
                # ===================
                self.generator.train()
                self.optimizer_G.zero_grad()
                
                # ç”Ÿæˆå‚æ•°
                pred_params_norm = self.generator(real_spectrum)
                pred_params_denorm = denormalize_params(pred_params_norm, dataset.param_ranges)
                
                # å¯¹æŠ—æŸå¤± (å¤§å¹…é™ä½æƒé‡)
                if epoch >= self.emergency_config['warmup_epochs']:
                    gen_scores = self.discriminator(real_spectrum, pred_params_denorm)
                    real_labels = torch.ones(batch_size, 1).to(self.device)
                    g_loss_adv = self.criterion_bce(gen_scores, real_labels)
                    adversarial_weight = self.emergency_config['adversarial_weight']
                else:
                    g_loss_adv = torch.tensor(0.0).to(self.device)
                    adversarial_weight = 0.0  # warmupæœŸé—´ä¸ä½¿ç”¨å¯¹æŠ—æŸå¤±
                
                # é‡å»ºæŸå¤± (æé«˜æƒé‡)
                g_loss_recon = self.criterion_mse(pred_params_norm, real_params_norm)
                g_loss_recon_l1 = self.criterion_l1(pred_params_norm, real_params_norm)
                
                # å‰å‘ä¸€è‡´æ€§æŸå¤± (å¤§å¹…æé«˜æƒé‡)
                pred_spectrum_from_params, _ = self.forward_model(pred_params_norm)
                g_loss_consistency = self.criterion_mse(pred_spectrum_from_params, real_spectrum)
                
                # æ€»ç”Ÿæˆå™¨æŸå¤±
                g_loss = (
                    adversarial_weight * g_loss_adv +
                    self.emergency_config['reconstruction_weight'] * g_loss_recon +
                    self.emergency_config['l1_penalty_weight'] * g_loss_recon_l1 +
                    self.emergency_config['forward_consistency_weight'] * g_loss_consistency
                )
                
                g_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.generator.parameters(), 0.5)
                self.optimizer_G.step()
                
                # è®¡ç®—RÂ²åˆ†æ•°
                with torch.no_grad():
                    real_flat = real_params_norm.cpu().numpy().flatten()
                    pred_flat = pred_params_norm.detach().cpu().numpy().flatten()
                    
                    # ç®€å•RÂ²è®¡ç®—
                    ss_res = np.sum((real_flat - pred_flat) ** 2)
                    ss_tot = np.sum((real_flat - np.mean(real_flat)) ** 2)
                    r2_score = 1 - (ss_res / (ss_tot + 1e-8))
                
                # ===================
                # è®­ç»ƒåˆ¤åˆ«å™¨ (é™ä½é¢‘ç‡)
                # ===================
                discriminator_update_counter += 1
                if discriminator_update_counter % self.emergency_config['discriminator_update_freq'] == 0:
                    self.discriminator.train()
                    self.optimizer_D.zero_grad()
                    
                    # çœŸå®æ ·æœ¬
                    real_labels = torch.ones(batch_size, 1).to(self.device) * 0.9  # æ ‡ç­¾å¹³æ»‘
                    real_scores = self.discriminator(real_spectrum, real_params_denorm)
                    d_loss_real = self.criterion_bce(real_scores, real_labels)
                    
                    # ç”Ÿæˆæ ·æœ¬
                    fake_labels = torch.zeros(batch_size, 1).to(self.device) + 0.1  # æ ‡ç­¾å¹³æ»‘
                    with torch.no_grad():
                        fake_params_norm = self.generator(real_spectrum)
                    fake_params_denorm = denormalize_params(fake_params_norm, dataset.param_ranges)
                    fake_scores = self.discriminator(real_spectrum, fake_params_denorm)
                    d_loss_fake = self.criterion_bce(fake_scores, fake_labels)
                    
                    d_loss = (d_loss_real + d_loss_fake) / 2
                    d_loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 0.5)
                    self.optimizer_D.step()
                else:
                    d_loss = torch.tensor(0.0)
                
                # ç´¯ç§¯æŸå¤±
                epoch_losses['g_loss'] += g_loss.item()
                epoch_losses['d_loss'] += d_loss.item()
                epoch_losses['consistency_error'] += g_loss_consistency.item()
                epoch_losses['r2_score'] += r2_score
            
            # å¹³å‡æŸå¤±
            for key in epoch_losses:
                epoch_losses[key] /= len(dataloader)
            
            # è®°å½•å†å²
            self.train_history['g_losses'].append(epoch_losses['g_loss'])
            self.train_history['d_losses'].append(epoch_losses['d_loss'])
            self.train_history['consistency_errors'].append(epoch_losses['consistency_error'])
            self.train_history['r2_scores'].append(epoch_losses['r2_score'])
            self.train_history['lr_history']['generator'].append(
                self.optimizer_G.param_groups[0]['lr']
            )
            self.train_history['lr_history']['discriminator'].append(
                self.optimizer_D.param_groups[0]['lr']
            )
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler_G.step(epoch_losses['g_loss'])
            self.scheduler_D.step(epoch_losses['d_loss'])
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{num_epochs}]")
                print(f"  G Loss: {epoch_losses['g_loss']:.6f}")
                print(f"  D Loss: {epoch_losses['d_loss']:.6f}")
                print(f"  Consistency Error: {epoch_losses['consistency_error']:.6f}")
                print(f"  RÂ² Score: {epoch_losses['r2_score']:.4f}")
                print(f"  G LR: {self.optimizer_G.param_groups[0]['lr']:.6f}")
                
                # åˆ¤æ–­è®­ç»ƒçŠ¶æ€
                if epoch_losses['r2_score'] > 0.1:
                    print("  ğŸŸ¢ RÂ²åˆ†æ•°å¼€å§‹æ”¹å–„")
                elif epoch_losses['r2_score'] > -0.1:
                    print("  ğŸŸ¡ RÂ²åˆ†æ•°æ¥è¿‘æ­£å€¼")
                else:
                    print("  ğŸ”´ RÂ²åˆ†æ•°ä»ä¸ºè´Ÿå€¼")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 50 == 0:
                self.save_checkpoint(epoch + 1, "emergency")
        
        print("âœ“ Balanced GAN training completed")
        
    def emergency_full_training(self, dataloader, dataset):
        """
        å®Œæ•´ç´§æ€¥ä¿®å¤è®­ç»ƒæµç¨‹
        """
        print(f"\n{'='*80}")
        print("ğŸš¨ EMERGENCY TRAINING PIPELINE")
        print(f"{'='*80}")
        print("é˜¶æ®µ1: å‰å‘ç½‘ç»œå¼ºåŒ–è®­ç»ƒ (200è½®)")
        print("é˜¶æ®µ2: å¹³è¡¡GANè®­ç»ƒ (200è½®)")
        print(f"{'='*80}")
        
        start_time = time.time()
        
        # é˜¶æ®µ1: å‰å‘ç½‘ç»œå¼ºåŒ–è®­ç»ƒ
        self.intensive_forward_training(dataloader, self.emergency_config['forward_intensive_epochs'])
        
        # é˜¶æ®µ2: å¹³è¡¡GANè®­ç»ƒ
        self.balanced_gan_training(dataloader, dataset, 200)
        
        total_time = time.time() - start_time
        print(f"\n{'='*80}")
        print(f"ğŸš¨ EMERGENCY TRAINING COMPLETED in {total_time:.2f}s ({total_time/60:.1f}min)")
        print(f"{'='*80}")
        
        # ç”Ÿæˆè®­ç»ƒæ›²çº¿
        self.plot_emergency_curves()
        
    def plot_emergency_curves(self):
        """ç”Ÿæˆç´§æ€¥è®­ç»ƒæ›²çº¿"""
        print("Generating emergency training curves...")
        
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(3, 2, figsize=(15, 18))
        fig.suptitle('Emergency Training Progress - Problem Solving', fontsize=16)
        
        # 1. å‰å‘æ¨¡å‹æŸå¤±
        if self.train_history['forward_losses']:
            forward_epochs = range(1, len(self.train_history['forward_losses']) + 1)
            axes[0, 0].plot(forward_epochs, self.train_history['forward_losses'], 'b-', linewidth=2)
            axes[0, 0].set_title('Forward Model Intensive Training')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].grid(True, alpha=0.3)
            axes[0, 0].set_yscale('log')
        
        # 2. GANæŸå¤±
        if self.train_history['g_losses']:
            gan_epochs = range(1, len(self.train_history['g_losses']) + 1)
            axes[0, 1].plot(gan_epochs, self.train_history['g_losses'], 'b-', label='Generator', linewidth=2)
            axes[0, 1].plot(gan_epochs, self.train_history['d_losses'], 'r-', label='Discriminator', linewidth=2)
            axes[0, 1].set_title('Balanced GAN Training')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ä¸€è‡´æ€§è¯¯å·®æ”¹å–„
        if self.train_history['consistency_errors']:
            axes[1, 0].plot(gan_epochs, self.train_history['consistency_errors'], 'orange', linewidth=2)
            axes[1, 0].axhline(y=0.01, color='green', linestyle='--', label='Target (<0.01)')
            axes[1, 0].set_title('Cycle Consistency Error Improvement')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Consistency Error')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
            axes[1, 0].set_yscale('log')
        
        # 4. RÂ²åˆ†æ•°æ”¹å–„
        if self.train_history['r2_scores']:
            axes[1, 1].plot(gan_epochs, self.train_history['r2_scores'], 'purple', linewidth=2)
            axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5, label='Zero Line')
            axes[1, 1].axhline(y=0.8, color='green', linestyle='--', label='Target (>0.8)')
            axes[1, 1].set_title('Parameter Prediction RÂ² Recovery')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('RÂ² Score')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        # 5. å­¦ä¹ ç‡è°ƒåº¦
        if self.train_history['lr_history']['forward_model']:
            axes[2, 0].plot(forward_epochs, self.train_history['lr_history']['forward_model'], 
                           'g-', label='Forward Model', linewidth=2)
            if self.train_history['lr_history']['generator']:
                axes[2, 0].plot(gan_epochs, self.train_history['lr_history']['generator'], 
                               'b-', label='Generator', linewidth=2)
                axes[2, 0].plot(gan_epochs, self.train_history['lr_history']['discriminator'], 
                               'r-', label='Discriminator (Reduced)', linewidth=2)
            axes[2, 0].set_title('Learning Rate Schedules')
            axes[2, 0].set_xlabel('Epoch')
            axes[2, 0].set_ylabel('Learning Rate')
            axes[2, 0].set_yscale('log')
            axes[2, 0].legend()
            axes[2, 0].grid(True, alpha=0.3)
        
        # 6. é—®é¢˜è§£å†³æ€»ç»“
        axes[2, 1].axis('off')
        
        summary_text = """
Emergency Training Summary:

ğŸ”´ Identified Problems:
â€¢ Forward Network RÂ²: -0.1768 (Collapsed)
â€¢ Generator RÂ²: -0.3637 (Collapsed)  
â€¢ Cycle Consistency: 0.2062 (Very Poor)
â€¢ Discriminator: 0.9225 (Too Strong)

ğŸŸ¢ Solutions Applied:
â€¢ Intensive forward training (200 epochs)
â€¢ Reduced discriminator learning rate (5x)
â€¢ Increased consistency loss weight (20x)
â€¢ Added L1 regularization
â€¢ Progressive adversarial training
â€¢ Balanced update frequencies

ğŸ¯ Expected Improvements:
â€¢ Forward RÂ²: -0.18 â†’ >0.80
â€¢ Generator RÂ²: -0.36 â†’ >0.70  
â€¢ Consistency: 0.21 â†’ <0.05
â€¢ Balanced discriminator accuracy
"""
        
        axes[2, 1].text(0.05, 0.95, summary_text, transform=axes[2, 1].transAxes,
                       fontsize=10, verticalalignment='top', fontfamily='monospace')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plots_dir = os.path.join(cfg.PROJECT_ROOT, "plots")
        os.makedirs(plots_dir, exist_ok=True)
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        plot_path = os.path.join(plots_dir, f"emergency_training_curves_{timestamp}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ Emergency training curves saved to: {plot_path}")
    
    def save_checkpoint(self, epoch: int, mode: str = "emergency"):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'mode': mode,
            'train_history': self.train_history,
            'emergency_config': self.emergency_config,
            'generator_state_dict': self.generator.state_dict(),
            'discriminator_state_dict': self.discriminator.state_dict(),
            'forward_model_state_dict': self.forward_model.state_dict(),
        }
        
        if self.optimizer_G is not None:
            checkpoint['optimizer_G_state_dict'] = self.optimizer_G.state_dict()
        if self.optimizer_D is not None:
            checkpoint['optimizer_D_state_dict'] = self.optimizer_D.state_dict()
        if self.optimizer_F is not None:
            checkpoint['optimizer_F_state_dict'] = self.optimizer_F.state_dict()
        
        os.makedirs(cfg.CHECKPOINT_DIR, exist_ok=True)
        checkpoint_path = os.path.join(cfg.CHECKPOINT_DIR, f'emergency_checkpoint_epoch_{epoch}.pth')
        torch.save(checkpoint, checkpoint_path)
        print(f"Emergency checkpoint saved: {checkpoint_path}")
    
    def save_final_models(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        os.makedirs(cfg.SAVED_MODELS_DIR, exist_ok=True)
        
        # ä¿å­˜ä¸ºè¯„ä¼°å™¨æœŸæœ›çš„æ–‡ä»¶å
        torch.save(self.generator.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "generator_final.pth"))
        torch.save(self.discriminator.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "discriminator_final.pth"))
        torch.save(self.forward_model.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "forward_model_final.pth"))
        
        # åŒæ—¶ä¿å­˜ç´§æ€¥ä¿®å¤ç‰ˆæœ¬
        torch.save(self.generator.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "generator_emergency.pth"))
        torch.save(self.discriminator.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "discriminator_emergency.pth"))
        torch.save(self.forward_model.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "forward_model_emergency.pth"))
        
        print("âœ“ Emergency training models saved")
        print(f"âœ“ Models saved to: {cfg.SAVED_MODELS_DIR}")
        print("  - *_final.pth (for evaluator)")
        print("  - *_emergency.pth (backup)")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Emergency PI-GAN Training - Problem Solver")
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size (reduced for stability)')
    parser.add_argument('--device', type=str, default='auto', help='Device to use')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = MetamaterialDataset(cfg.DATASET_PATH, cfg.SPECTRUM_DIM)
    dataloader = torch.utils.data.DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )
    
    print(f"Emergency training dataset: {len(dataset)} samples")
    print(f"Batch size: {args.batch_size} (reduced for stability)")
    
    # åˆ›å»ºç´§æ€¥è®­ç»ƒå™¨
    trainer = EmergencyTrainer(device=args.device)
    trainer.initialize_models()
    trainer.initialize_optimizers()
    
    # å¼€å§‹ç´§æ€¥ä¿®å¤è®­ç»ƒ
    trainer.emergency_full_training(dataloader, dataset)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_final_models()
    
    print(f"\nğŸš¨ Emergency training completed!")
    print("ğŸ” å»ºè®®ç«‹å³è¿è¡Œè¯„ä¼°éªŒè¯æ”¹è¿›æ•ˆæœ:")
    print("python core/evaluate/unified_evaluator.py --num_samples 1000")

if __name__ == "__main__":
    main()