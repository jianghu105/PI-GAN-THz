# PI_GAN_THZ/core/train/constraint_optimizer.py
# çº¦æŸä¼˜åŒ–è®­ç»ƒå™¨ - ä¸“é—¨è§£å†³å‚æ•°è¿çº¦ç‡é—®é¢˜

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np
import argparse
import time
import matplotlib.pyplot as plt
from typing import Dict, Tuple

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

from core.models.generator import Generator
from core.models.discriminator import Discriminator
from core.models.forward_model import ForwardModel
from core.utils.data_loader import MetamaterialDataset, denormalize_params, denormalize_metrics
from core.utils.set_seed import set_seed
from core.utils.loss import criterion_bce, criterion_mse
import config.config as cfg

class ConstraintOptimizer:
    """
    çº¦æŸä¼˜åŒ–è®­ç»ƒå™¨ - ä¸“é—¨è§£å†³91.4%å‚æ•°è¿çº¦ç‡é—®é¢˜
    
    å½“å‰çŠ¶æ€:
    âœ“ PI-GAN Parameter RÂ²: 0.9888 (ä¼˜ç§€)
    âœ“ Cycle Consistency: 0.013182 (ä¼˜ç§€)
    âœ“ Discriminator Balance: 51.00% (å®Œç¾)
    âŒ Parameter Violation Rate: 91.4% (éœ€è¦è§£å†³)
    """
    
    def __init__(self, device: str = "auto"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu") if device == "auto" else torch.device(device)
        
        # æ¨¡å‹
        self.generator = None
        self.discriminator = None
        self.forward_model = None
        
        # ä¼˜åŒ–å™¨
        self.optimizer_G = None
        self.optimizer_D = None
        
        # æŸå¤±å‡½æ•°
        self.criterion_bce = criterion_bce()
        self.criterion_mse = criterion_mse()
        self.criterion_l1 = nn.L1Loss()
        
        # çº¦æŸä¼˜åŒ–ä¸“ç”¨é…ç½®
        self.constraint_config = {
            # çº¦æŸç›¸å…³æƒé‡ - å¤§å¹…æå‡
            'hard_constraint_weight': 50.0,      # ç¡¬çº¦æŸæƒé‡
            'boundary_penalty_weight': 20.0,     # è¾¹ç•Œæƒ©ç½šæƒé‡
            'range_violation_weight': 100.0,     # èŒƒå›´è¿çº¦æƒ©ç½š
            'smoothness_penalty_weight': 10.0,   # å‚æ•°å¹³æ»‘æ€§
            
            # ä¿æŒç°æœ‰ä¼˜ç§€æ€§èƒ½çš„æƒé‡
            'reconstruction_weight': 15.0,       # ä¿æŒé«˜é‡å»ºæ€§èƒ½
            'consistency_weight': 20.0,          # ä¿æŒå¾ªç¯ä¸€è‡´æ€§
            'adversarial_weight': 0.1,           # ä¿æŒåˆ¤åˆ«å™¨å¹³è¡¡
            
            # å­¦ä¹ ç‡é…ç½® - ç²¾ç»†è°ƒæ•´
            'generator_lr': 1e-4,                # ç”Ÿæˆå™¨å­¦ä¹ ç‡
            'discriminator_lr': 5e-5,            # ä¿æŒåˆ¤åˆ«å™¨å¹³è¡¡
            
            # çº¦æŸè®­ç»ƒç­–ç•¥
            'constraint_epochs': 100,            # ä¸“é—¨çš„çº¦æŸä¼˜åŒ–è½®æ•°
            'constraint_warmup': 20,             # çº¦æŸé¢„çƒ­è½®æ•°
            'constraint_annealing': True,        # çº¦æŸæƒé‡é€’å¢
        }
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'g_losses': [],
            'd_losses': [],
            'violation_rates': [],
            'constraint_losses': [],
            'reconstruction_losses': [],
            'r2_scores': []
        }
        
        print(f"Constraint Optimizer initialized on device: {self.device}")
        print("ğŸ¯ ä¸“é—¨è§£å†³å‚æ•°è¿çº¦ç‡é—®é¢˜ (å½“å‰91.4% â†’ ç›®æ ‡<10%)")
    
    def load_pretrained_models(self):
        """åŠ è½½ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹"""
        print("Loading existing models...")
        
        # åˆå§‹åŒ–æ¨¡å‹ç»“æ„
        self.generator = Generator(
            input_dim=cfg.SPECTRUM_DIM,
            output_dim=cfg.GENERATOR_OUTPUT_PARAM_DIM
        ).to(self.device)
        
        self.discriminator = Discriminator(
            input_spec_dim=cfg.DISCRIMINATOR_INPUT_SPEC_DIM,
            input_param_dim=cfg.DISCRIMINATOR_INPUT_PARAM_DIM
        ).to(self.device)
        
        self.forward_model = ForwardModel(
            input_param_dim=cfg.FORWARD_MODEL_INPUT_DIM,
            output_spectrum_dim=cfg.FORWARD_MODEL_OUTPUT_SPEC_DIM,
            output_metrics_dim=cfg.FORWARD_MODEL_OUTPUT_METRICS_DIM
        ).to(self.device)
        
        # åŠ è½½ç°æœ‰æƒé‡
        try:
            self.generator.load_state_dict(
                torch.load(os.path.join(cfg.SAVED_MODELS_DIR, "generator_final.pth"), 
                          map_location=self.device)
            )
            self.discriminator.load_state_dict(
                torch.load(os.path.join(cfg.SAVED_MODELS_DIR, "discriminator_final.pth"), 
                          map_location=self.device)
            )
            self.forward_model.load_state_dict(
                torch.load(os.path.join(cfg.SAVED_MODELS_DIR, "forward_model_final.pth"), 
                          map_location=self.device)
            )
            print("âœ“ Successfully loaded pretrained models")
            print("  Current performance: RÂ²=0.9888, Consistency=0.013, Balance=51%")
            print("  Focus: Reduce violation rate from 91.4% to <10%")
            
        except Exception as e:
            print(f"âœ— Error loading models: {e}")
            print("Please ensure models exist in saved_models/ directory")
            return False
        
        return True
    
    def initialize_optimizers(self):
        """åˆå§‹åŒ–çº¦æŸä¼˜åŒ–ä¸“ç”¨ä¼˜åŒ–å™¨"""
        print("Initializing constraint optimization optimizers...")
        
        # ç”Ÿæˆå™¨ - ä¸“æ³¨äºçº¦æŸä¼˜åŒ–
        self.optimizer_G = optim.Adam(
            self.generator.parameters(),
            lr=self.constraint_config['generator_lr'],
            betas=(0.9, 0.999),
            weight_decay=1e-4
        )
        
        # åˆ¤åˆ«å™¨ - ä¿æŒå½“å‰å¹³è¡¡
        self.optimizer_D = optim.Adam(
            self.discriminator.parameters(),
            lr=self.constraint_config['discriminator_lr'],
            betas=(0.5, 0.999),
            weight_decay=1e-4
        )
        
        print("âœ“ Constraint optimization optimizers initialized")
        print(f"  - Generator LR: {self.constraint_config['generator_lr']} (focused on constraints)")
        print(f"  - Discriminator LR: {self.constraint_config['discriminator_lr']} (maintain balance)")
    
    def calculate_enhanced_constraint_loss(self, pred_params_norm: torch.Tensor, param_ranges: Dict) -> Dict:
        """è®¡ç®—å¢å¼ºçš„çº¦æŸæŸå¤±"""
        batch_size = pred_params_norm.size(0)
        
        # 1. ç¡¬çº¦æŸæŸå¤± - èŒƒå›´è¿çº¦ä¸¥å‰æƒ©ç½š
        range_violations = torch.sum(
            torch.relu(pred_params_norm - 1.0) + torch.relu(-pred_params_norm)
        )
        hard_constraint_loss = range_violations / batch_size
        
        # 2. è¾¹ç•Œæƒ©ç½š - é˜²æ­¢å‚æ•°è´´è¾¹ç•Œ
        boundary_distance = torch.min(pred_params_norm, 1.0 - pred_params_norm)
        boundary_penalty = torch.mean(torch.exp(-10 * boundary_distance))
        
        # 3. å¹³æ»‘æ€§çº¦æŸ - å‚æ•°é—´çš„å¹³æ»‘æ€§
        if pred_params_norm.size(1) > 1:
            param_diff = torch.diff(pred_params_norm, dim=1)
            smoothness_loss = torch.mean(param_diff ** 2)
        else:
            smoothness_loss = torch.tensor(0.0).to(self.device)
        
        # 4. ç‰©ç†åˆç†æ€§çº¦æŸ - åŸºäºå‰å‘æ¨¡å‹éªŒè¯
        with torch.no_grad():
            pred_spectrum, pred_metrics = self.forward_model(pred_params_norm)
            # æ£€æŸ¥ç”Ÿæˆçš„å…‰è°±æ˜¯å¦åˆç†
            spectrum_validity = torch.mean(torch.relu(-pred_spectrum))  # è´Ÿå€¼æƒ©ç½š
        
        return {
            'hard_constraint': hard_constraint_loss,
            'boundary_penalty': boundary_penalty,
            'smoothness': smoothness_loss,
            'spectrum_validity': spectrum_validity
        }
    
    def calculate_violation_rate(self, pred_params_norm: torch.Tensor) -> float:
        """è®¡ç®—å‚æ•°è¿çº¦ç‡"""
        violations = torch.logical_or(pred_params_norm < 0, pred_params_norm > 1)
        violation_rate = torch.mean(violations.float()).item()
        return violation_rate
    
    def constraint_focused_training(self, dataloader, dataset, num_epochs: int = 100):
        """ä¸“æ³¨äºçº¦æŸçš„è®­ç»ƒ"""
        print(f"\nğŸ¯ CONSTRAINT-FOCUSED TRAINING ({num_epochs} epochs)")
        print("ç›®æ ‡: å‚æ•°è¿çº¦ç‡ 91.4% â†’ <10%")
        print("ç­–ç•¥: ä¿æŒç°æœ‰ä¼˜ç§€æ€§èƒ½ï¼Œä¸“æ³¨è§£å†³çº¦æŸé—®é¢˜")
        
        best_violation_rate = 1.0  # æœ€ä½³è¿çº¦ç‡
        
        for epoch in range(num_epochs):
            epoch_metrics = {
                'g_loss': 0.0,
                'd_loss': 0.0,
                'violation_rate': 0.0,
                'constraint_loss': 0.0,
                'reconstruction_loss': 0.0,
                'r2_score': 0.0
            }
            
            # åŠ¨æ€çº¦æŸæƒé‡ - éšè®­ç»ƒè¿›è¡Œé€’å¢
            if self.constraint_config['constraint_annealing']:
                constraint_multiplier = min(1.0 + epoch / 50.0, 3.0)  # é€æ¸å¢å¼ºçº¦æŸ
            else:
                constraint_multiplier = 1.0
            
            for batch_idx, batch in enumerate(dataloader):
                real_spectrum, real_params_denorm, real_params_norm, _, _ = batch
                
                real_spectrum = real_spectrum.to(self.device)
                real_params_denorm = real_params_denorm.to(self.device)
                real_params_norm = real_params_norm.to(self.device)
                
                batch_size = real_spectrum.size(0)
                
                # ===================
                # è®­ç»ƒç”Ÿæˆå™¨ - ä¸“æ³¨çº¦æŸä¼˜åŒ–
                # ===================
                self.generator.train()
                self.optimizer_G.zero_grad()
                
                # ç”Ÿæˆå‚æ•°
                pred_params_norm = self.generator(real_spectrum)
                
                # åº”ç”¨ sigmoid ç¡®ä¿èŒƒå›´çº¦æŸ
                pred_params_norm = torch.sigmoid(pred_params_norm)
                
                pred_params_denorm = denormalize_params(pred_params_norm, dataset.param_ranges)
                
                # 1. å¢å¼ºçº¦æŸæŸå¤±
                constraint_losses = self.calculate_enhanced_constraint_loss(pred_params_norm, dataset.param_ranges)
                total_constraint_loss = (
                    self.constraint_config['hard_constraint_weight'] * constraint_losses['hard_constraint'] +
                    self.constraint_config['boundary_penalty_weight'] * constraint_losses['boundary_penalty'] +
                    self.constraint_config['smoothness_penalty_weight'] * constraint_losses['smoothness'] +
                    10.0 * constraint_losses['spectrum_validity']
                ) * constraint_multiplier
                
                # 2. ä¿æŒé‡å»ºæ€§èƒ½
                reconstruction_loss = self.criterion_mse(pred_params_norm, real_params_norm)
                
                # 3. ä¿æŒå‰å‘ä¸€è‡´æ€§
                pred_spectrum_from_params, _ = self.forward_model(pred_params_norm)
                consistency_loss = self.criterion_mse(pred_spectrum_from_params, real_spectrum)
                
                # 4. è½»å¾®å¯¹æŠ—æŸå¤± - ä¿æŒåˆ¤åˆ«å™¨å¹³è¡¡
                if epoch >= self.constraint_config['constraint_warmup']:
                    gen_scores = self.discriminator(real_spectrum, pred_params_denorm)
                    real_labels = torch.ones(batch_size, 1).to(self.device)
                    adversarial_loss = self.criterion_bce(gen_scores, real_labels)
                else:
                    adversarial_loss = torch.tensor(0.0).to(self.device)
                
                # æ€»ç”Ÿæˆå™¨æŸå¤± - çº¦æŸä¸ºä¸»å¯¼
                g_loss = (
                    total_constraint_loss +
                    self.constraint_config['reconstruction_weight'] * reconstruction_loss +
                    self.constraint_config['consistency_weight'] * consistency_loss +
                    self.constraint_config['adversarial_weight'] * adversarial_loss
                )
                
                g_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.generator.parameters(), 1.0)
                self.optimizer_G.step()
                
                # è®¡ç®—è¿çº¦ç‡
                violation_rate = self.calculate_violation_rate(pred_params_norm)
                
                # è®¡ç®—RÂ²åˆ†æ•°
                with torch.no_grad():
                    real_flat = real_params_norm.cpu().numpy().flatten()
                    pred_flat = pred_params_norm.detach().cpu().numpy().flatten()
                    ss_res = np.sum((real_flat - pred_flat) ** 2)
                    ss_tot = np.sum((real_flat - np.mean(real_flat)) ** 2)
                    r2_score = 1 - (ss_res / (ss_tot + 1e-8))
                
                # ===================
                # è½»é‡çº§åˆ¤åˆ«å™¨è®­ç»ƒ - ä¿æŒå¹³è¡¡
                # ===================
                if (batch_idx + 1) % 3 == 0:  # é™ä½æ›´æ–°é¢‘ç‡
                    self.discriminator.train()
                    self.optimizer_D.zero_grad()
                    
                    # çœŸå®æ ·æœ¬
                    real_labels = torch.ones(batch_size, 1).to(self.device) * 0.9
                    real_scores = self.discriminator(real_spectrum, real_params_denorm)
                    d_loss_real = self.criterion_bce(real_scores, real_labels)
                    
                    # ç”Ÿæˆæ ·æœ¬
                    fake_labels = torch.zeros(batch_size, 1).to(self.device) + 0.1
                    with torch.no_grad():
                        fake_params_norm = self.generator(real_spectrum)
                        fake_params_norm = torch.sigmoid(fake_params_norm)
                    fake_params_denorm = denormalize_params(fake_params_norm, dataset.param_ranges)
                    fake_scores = self.discriminator(real_spectrum, fake_params_denorm)
                    d_loss_fake = self.criterion_bce(fake_scores, fake_labels)
                    
                    d_loss = (d_loss_real + d_loss_fake) / 2
                    d_loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 0.5)
                    self.optimizer_D.step()
                else:
                    d_loss = torch.tensor(0.0)
                
                # ç´¯ç§¯æŒ‡æ ‡
                epoch_metrics['g_loss'] += g_loss.item()
                epoch_metrics['d_loss'] += d_loss.item()
                epoch_metrics['violation_rate'] += violation_rate
                epoch_metrics['constraint_loss'] += total_constraint_loss.item()
                epoch_metrics['reconstruction_loss'] += reconstruction_loss.item()
                epoch_metrics['r2_score'] += r2_score
            
            # å¹³å‡æŒ‡æ ‡
            for key in epoch_metrics:
                epoch_metrics[key] /= len(dataloader)
            
            # è®°å½•å†å²
            self.train_history['g_losses'].append(epoch_metrics['g_loss'])
            self.train_history['d_losses'].append(epoch_metrics['d_loss'])
            self.train_history['violation_rates'].append(epoch_metrics['violation_rate'])
            self.train_history['constraint_losses'].append(epoch_metrics['constraint_loss'])
            self.train_history['reconstruction_losses'].append(epoch_metrics['reconstruction_loss'])
            self.train_history['r2_scores'].append(epoch_metrics['r2_score'])
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if epoch_metrics['violation_rate'] < best_violation_rate:
                best_violation_rate = epoch_metrics['violation_rate']
                self.save_best_models()
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{num_epochs}]")
                print(f"  Violation Rate: {epoch_metrics['violation_rate']:.4f} (ç›®æ ‡<0.10)")
                print(f"  RÂ² Score: {epoch_metrics['r2_score']:.4f} (ä¿æŒ>0.98)")
                print(f"  G Loss: {epoch_metrics['g_loss']:.6f}")
                print(f"  Constraint Loss: {epoch_metrics['constraint_loss']:.6f}")
                print(f"  Best Violation Rate: {best_violation_rate:.4f}")
                
                # è¯„ä¼°çº¦æŸæ”¹å–„çŠ¶æ€
                if epoch_metrics['violation_rate'] < 0.10:
                    print("  ğŸ‰ çº¦æŸç›®æ ‡å·²è¾¾æˆï¼")
                elif epoch_metrics['violation_rate'] < 0.30:
                    print("  ğŸŸ¢ çº¦æŸæ˜¾è‘—æ”¹å–„")
                elif epoch_metrics['violation_rate'] < 0.60:
                    print("  ğŸŸ¡ çº¦æŸæŒç»­æ”¹å–„")
                else:
                    print("  ğŸ”´ çº¦æŸä»éœ€åŠªåŠ›")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 25 == 0:
                self.save_checkpoint(epoch + 1, "constraint")
        
        print(f"\nâœ“ Constraint optimization completed")
        print(f"  Final Violation Rate: {epoch_metrics['violation_rate']:.4f}")
        print(f"  Best Violation Rate: {best_violation_rate:.4f}")
        print(f"  Final RÂ² Score: {epoch_metrics['r2_score']:.4f}")
    
    def save_best_models(self):
        """ä¿å­˜æœ€ä½³çº¦æŸä¼˜åŒ–æ¨¡å‹"""
        os.makedirs(cfg.SAVED_MODELS_DIR, exist_ok=True)
        
        torch.save(self.generator.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "generator_final.pth"))
        torch.save(self.discriminator.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "discriminator_final.pth"))
        torch.save(self.forward_model.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "forward_model_final.pth"))
        
        # åŒæ—¶ä¿å­˜çº¦æŸä¼˜åŒ–ç‰ˆæœ¬
        torch.save(self.generator.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "generator_constraint_opt.pth"))
        torch.save(self.discriminator.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "discriminator_constraint_opt.pth"))
        torch.save(self.forward_model.state_dict(), 
                  os.path.join(cfg.SAVED_MODELS_DIR, "forward_model_constraint_opt.pth"))
    
    def save_checkpoint(self, epoch: int, mode: str = "constraint"):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'mode': mode,
            'train_history': self.train_history,
            'constraint_config': self.constraint_config,
            'generator_state_dict': self.generator.state_dict(),
            'discriminator_state_dict': self.discriminator.state_dict(),
            'forward_model_state_dict': self.forward_model.state_dict(),
        }
        
        if self.optimizer_G is not None:
            checkpoint['optimizer_G_state_dict'] = self.optimizer_G.state_dict()
        if self.optimizer_D is not None:
            checkpoint['optimizer_D_state_dict'] = self.optimizer_D.state_dict()
        
        os.makedirs(cfg.CHECKPOINT_DIR, exist_ok=True)
        checkpoint_path = os.path.join(cfg.CHECKPOINT_DIR, f'constraint_checkpoint_epoch_{epoch}.pth')
        torch.save(checkpoint, checkpoint_path)
    
    def plot_constraint_optimization_curves(self):
        """ç”Ÿæˆçº¦æŸä¼˜åŒ–è®­ç»ƒæ›²çº¿"""
        print("Generating constraint optimization curves...")
        
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Constraint Optimization Progress - Violation Rate Reduction', fontsize=16)
        
        epochs = range(1, len(self.train_history['violation_rates']) + 1)
        
        # 1. è¿çº¦ç‡æ”¹å–„ - æœ€é‡è¦
        axes[0, 0].plot(epochs, self.train_history['violation_rates'], 'r-', linewidth=3, label='Violation Rate')
        axes[0, 0].axhline(y=0.10, color='green', linestyle='--', linewidth=2, label='Target (<10%)')
        axes[0, 0].axhline(y=0.914, color='gray', linestyle=':', label='Initial (91.4%)')
        axes[0, 0].set_title('Parameter Violation Rate Reduction', fontweight='bold')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Violation Rate')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_ylim(0, 1)
        
        # 2. RÂ²æ€§èƒ½ä¿æŒ
        axes[0, 1].plot(epochs, self.train_history['r2_scores'], 'b-', linewidth=2, label='RÂ² Score')
        axes[0, 1].axhline(y=0.9888, color='green', linestyle='--', label='Initial Performance')
        axes[0, 1].axhline(y=0.80, color='orange', linestyle=':', label='Minimum Target')
        axes[0, 1].set_title('RÂ² Score Maintenance', fontweight='bold')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('RÂ² Score')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. çº¦æŸæŸå¤±å’Œé‡å»ºæŸå¤±
        axes[1, 0].plot(epochs, self.train_history['constraint_losses'], 'orange', linewidth=2, label='Constraint Loss')
        axes[1, 0].plot(epochs, self.train_history['reconstruction_losses'], 'purple', linewidth=2, label='Reconstruction Loss')
        axes[1, 0].set_title('Loss Components')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Loss')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].set_yscale('log')
        
        # 4. GANæŸå¤±å¹³è¡¡
        axes[1, 1].plot(epochs, self.train_history['g_losses'], 'b-', linewidth=2, label='Generator')
        axes[1, 1].plot(epochs, self.train_history['d_losses'], 'r-', linewidth=2, label='Discriminator')
        axes[1, 1].set_title('GAN Loss Balance (Maintain ~51% D Accuracy)')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Loss')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plots_dir = os.path.join(cfg.PROJECT_ROOT, "plots")
        os.makedirs(plots_dir, exist_ok=True)
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        plot_path = os.path.join(plots_dir, f"constraint_optimization_curves_{timestamp}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ Constraint optimization curves saved to: {plot_path}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Constraint Optimization - Fix Parameter Violations")
    parser.add_argument('--epochs', type=int, default=100, help='Constraint optimization epochs')
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')
    parser.add_argument('--device', type=str, default='auto', help='Device to use')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = MetamaterialDataset(cfg.DATASET_PATH, cfg.SPECTRUM_DIM)
    dataloader = torch.utils.data.DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )
    
    print(f"Constraint optimization dataset: {len(dataset)} samples")
    print(f"Batch size: {args.batch_size}")
    
    # åˆ›å»ºçº¦æŸä¼˜åŒ–å™¨
    optimizer = ConstraintOptimizer(device=args.device)
    
    # åŠ è½½ç°æœ‰æ¨¡å‹
    if not optimizer.load_pretrained_models():
        print("Failed to load pretrained models. Exiting...")
        return
    
    optimizer.initialize_optimizers()
    
    # å¼€å§‹çº¦æŸä¼˜åŒ–è®­ç»ƒ
    print(f"\n{'='*80}")
    print("ğŸ¯ CONSTRAINT OPTIMIZATION TRAINING")
    print(f"{'='*80}")
    print("ç›®æ ‡: å‚æ•°è¿çº¦ç‡ 91.4% â†’ <10%")
    print("ä¿æŒ: RÂ²=0.9888, ä¸€è‡´æ€§=0.013, åˆ¤åˆ«å™¨å¹³è¡¡=51%")
    print(f"{'='*80}")
    
    optimizer.constraint_focused_training(dataloader, dataset, args.epochs)
    
    # ç”Ÿæˆè®­ç»ƒæ›²çº¿
    optimizer.plot_constraint_optimization_curves()
    
    print(f"\nğŸ¯ Constraint optimization completed!")
    print("ğŸ” å»ºè®®ç«‹å³è¿è¡Œè¯„ä¼°éªŒè¯çº¦æŸæ”¹å–„æ•ˆæœ:")
    print("python3 core/evaluate/unified_evaluator.py --num_samples 1000")

if __name__ == "__main__":
    main()