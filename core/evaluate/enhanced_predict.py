# PI_GAN_THZ/core/evaluate/enhanced_predict.py

import sys
import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional, Any, Union
from scipy.interpolate import interp1d
from scipy.signal import savgol_filter
import argparse
import time

# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

from core.models.generator import Generator
from core.models.forward_model import ForwardModel
import config.config as cfg
from core.utils.data_loader import MetamaterialDataset, denormalize_params, denormalize_metrics, normalize_spectrum
from core.utils.set_seed import set_seed
from core.utils.loss import criterion_mse

class EnhancedPredictor:
    """
    增强型预测器：提供更强大的结构预测功能
    """
    
    def __init__(self, device: str = "auto"):
        """
        初始化预测器
        
        Args:
            device: 计算设备 ("auto", "cpu", "cuda")
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu") if device == "auto" else torch.device(device)
        self.generator = None
        self.forward_model = None
        self.dataset = None
        
        print(f"Enhanced Predictor initialized on device: {self.device}")
    
    def load_models(self, model_dir: str = None) -> bool:
        """
        加载训练好的模型
        
        Args:
            model_dir: 模型保存目录
            
        Returns:
            bool: 加载是否成功
        """
        if model_dir is None:
            model_dir = cfg.SAVED_MODELS_DIR
            
        print(f"Loading models from: {model_dir}")
        
        try:
            # 初始化模型
            self.generator = Generator(
                input_dim=cfg.SPECTRUM_DIM, 
                output_dim=cfg.GENERATOR_OUTPUT_PARAM_DIM
            ).to(self.device)
            
            self.forward_model = ForwardModel(
                input_param_dim=cfg.FORWARD_MODEL_INPUT_DIM,
                output_spectrum_dim=cfg.FORWARD_MODEL_OUTPUT_SPEC_DIM,
                output_metrics_dim=cfg.FORWARD_MODEL_OUTPUT_METRICS_DIM
            ).to(self.device)
            
            # 加载权重
            gen_path = os.path.join(model_dir, "generator_final.pth")
            fwd_path = os.path.join(model_dir, "forward_model_final.pth")
            
            if not all(os.path.exists(p) for p in [gen_path, fwd_path]):
                print("Error: Required model files not found!")
                return False
                
            self.generator.load_state_dict(torch.load(gen_path, map_location=self.device))
            self.forward_model.load_state_dict(torch.load(fwd_path, map_location=self.device))
            
            # 设置为评估模式
            self.generator.eval()
            self.forward_model.eval()
            
            print("Models loaded successfully!")
            return True
            
        except Exception as e:
            print(f"Error loading models: {e}")
            return False
    
    def load_dataset(self, data_path: str = None) -> bool:
        """
        加载数据集（用于获取归一化参数）
        
        Args:
            data_path: 数据集路径
            
        Returns:
            bool: 加载是否成功
        """
        if data_path is None:
            data_path = cfg.DATASET_PATH
            
        try:
            self.dataset = MetamaterialDataset(
                data_path=data_path, 
                num_points_per_sample=cfg.SPECTRUM_DIM
            )\n            print(f\"Dataset loaded for normalization parameters\")\n            return True\n            \n        except Exception as e:\n            print(f\"Error loading dataset: {e}\")\n            return False\n    \n    def preprocess_spectrum_data(self, spectrum_data: Union[str, np.ndarray], \n                                target_frequencies: np.ndarray = None) -> np.ndarray:\n        \"\"\"\n        预处理光谱数据\n        \n        Args:\n            spectrum_data: 光谱数据文件路径或numpy数组\n            target_frequencies: 目标频率数组\n            \n        Returns:\n            np.ndarray: 预处理后的光谱数据\n        \"\"\"\n        if isinstance(spectrum_data, str):\n            # 从文件加载\n            if spectrum_data.endswith('.txt'):\n                spectrum = self._load_txt_spectrum(spectrum_data, target_frequencies)\n            elif spectrum_data.endswith('.npy'):\n                spectrum = np.load(spectrum_data)\n            elif spectrum_data.endswith('.csv'):\n                spectrum = pd.read_csv(spectrum_data).values\n            else:\n                raise ValueError(f\"Unsupported file format: {spectrum_data}\")\n        else:\n            spectrum = spectrum_data.copy()\n        \n        # 数据清理和预处理\n        spectrum = self._clean_spectrum_data(spectrum)\n        \n        # 插值到目标频率\n        if target_frequencies is not None and hasattr(self.dataset, 'frequencies'):\n            spectrum = self._interpolate_spectrum(spectrum, target_frequencies, self.dataset.frequencies)\n        \n        # 平滑滤波\n        spectrum = self._smooth_spectrum(spectrum)\n        \n        return spectrum\n    \n    def _load_txt_spectrum(self, file_path: str, target_frequencies: np.ndarray = None) -> np.ndarray:\n        \"\"\"\n        从TXT文件加载光谱数据\n        \n        Args:\n            file_path: 文件路径\n            target_frequencies: 目标频率数组\n            \n        Returns:\n            np.ndarray: 光谱数据\n        \"\"\"\n        try:\n            with open(file_path, 'r') as f:\n                lines = f.readlines()\n            \n            # 找到数据开始位置\n            data_start = 0\n            for i, line in enumerate(lines):\n                if line.startswith('#----') or line.startswith('Frequency'):\n                    data_start = i + 1\n                    break\n            \n            # 读取数据\n            data_lines = lines[data_start:]\n            data = []\n            for line in data_lines:\n                line = line.strip()\n                if line and not line.startswith('#'):\n                    parts = line.replace(',', '\\t').split('\\t')\n                    if len(parts) >= 2:\n                        try:\n                            freq = float(parts[0])\n                            s21_db = float(parts[1])\n                            data.append([freq, s21_db])\n                        except ValueError:\n                            continue\n            \n            if not data:\n                raise ValueError(\"No valid data found in file\")\n            \n            data = np.array(data)\n            frequencies = data[:, 0]\n            s21_db = data[:, 1]\n            \n            # 转换为线性尺度\n            spectrum = 10**(s21_db / 10.0)\n            \n            # 插值到目标频率\n            if target_frequencies is not None:\n                f_interp = interp1d(frequencies, spectrum, kind='linear', \n                                  bounds_error=False, fill_value='extrapolate')\n                spectrum = f_interp(target_frequencies)\n                spectrum = np.clip(spectrum, 0, None)  # 确保非负\n            \n            return spectrum\n            \n        except Exception as e:\n            raise ValueError(f\"Error loading TXT file {file_path}: {e}\")\n    \n    def _clean_spectrum_data(self, spectrum: np.ndarray) -> np.ndarray:\n        \"\"\"\n        清理光谱数据\n        \n        Args:\n            spectrum: 原始光谱数据\n            \n        Returns:\n            np.ndarray: 清理后的光谱数据\n        \"\"\"\n        # 移除NaN和无穷值\n        spectrum = np.nan_to_num(spectrum, nan=0.0, posinf=1.0, neginf=0.0)\n        \n        # 确保非负（如果是传输率数据）\n        if np.all(spectrum >= 0):\n            spectrum = np.maximum(spectrum, 0)\n        \n        # 移除异常值\n        q1, q3 = np.percentile(spectrum, [25, 75])\n        iqr = q3 - q1\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n        spectrum = np.clip(spectrum, lower_bound, upper_bound)\n        \n        return spectrum\n    \n    def _interpolate_spectrum(self, spectrum: np.ndarray, \n                            source_frequencies: np.ndarray, \n                            target_frequencies: np.ndarray) -> np.ndarray:\n        \"\"\"\n        插值光谱数据到目标频率\n        \n        Args:\n            spectrum: 光谱数据\n            source_frequencies: 源频率数组\n            target_frequencies: 目标频率数组\n            \n        Returns:\n            np.ndarray: 插值后的光谱数据\n        \"\"\"\n        try:\n            f_interp = interp1d(source_frequencies, spectrum, kind='cubic', \n                              bounds_error=False, fill_value='extrapolate')\n            interpolated = f_interp(target_frequencies)\n            return interpolated\n        except:\n            # 降级到线性插值\n            f_interp = interp1d(source_frequencies, spectrum, kind='linear', \n                              bounds_error=False, fill_value='extrapolate')\n            return f_interp(target_frequencies)\n    \n    def _smooth_spectrum(self, spectrum: np.ndarray, window_length: int = 5) -> np.ndarray:\n        \"\"\"\n        平滑光谱数据\n        \n        Args:\n            spectrum: 光谱数据\n            window_length: 滤波窗口长度\n            \n        Returns:\n            np.ndarray: 平滑后的光谱数据\n        \"\"\"\n        if len(spectrum) < window_length:\n            return spectrum\n        \n        try:\n            # 使用Savitzky-Golay滤波器\n            smoothed = savgol_filter(spectrum, window_length, 3)\n            return smoothed\n        except:\n            # 降级到简单移动平均\n            kernel = np.ones(window_length) / window_length\n            return np.convolve(spectrum, kernel, mode='same')\n    \n    def predict_structure(self, spectrum_input: Union[str, np.ndarray], \n                         uncertainty_samples: int = 0,\n                         optimize_prediction: bool = False) -> Dict[str, Any]:\n        \"\"\"\n        预测结构参数\n        \n        Args:\n            spectrum_input: 输入光谱（文件路径或数据数组）\n            uncertainty_samples: 不确定性采样次数\n            optimize_prediction: 是否进行预测优化\n            \n        Returns:\n            Dict[str, Any]: 预测结果\n        \"\"\"\n        print(\"Starting structure prediction...\")\n        start_time = time.time()\n        \n        if self.generator is None or self.forward_model is None or self.dataset is None:\n            raise ValueError(\"Models and dataset must be loaded first!\")\n        \n        # 预处理输入光谱\n        target_spectrum = self.preprocess_spectrum_data(spectrum_input)\n        \n        # 确保形状正确\n        if target_spectrum.ndim == 1:\n            target_spectrum = target_spectrum.reshape(1, -1)\n        \n        if target_spectrum.shape[1] != cfg.SPECTRUM_DIM:\n            raise ValueError(f\"Spectrum dimension mismatch: expected {cfg.SPECTRUM_DIM}, got {target_spectrum.shape[1]}\")\n        \n        # 归一化光谱\n        target_spectrum_tensor = torch.tensor(target_spectrum, dtype=torch.float32)\n        normalized_spectrum = normalize_spectrum(target_spectrum_tensor, \n                                               self.dataset.spectrum_min, \n                                               self.dataset.spectrum_max).to(self.device)\n        \n        results = {}\n        \n        with torch.no_grad():\n            if uncertainty_samples > 0:\n                # 蒙特卡洛不确定性估计\n                results.update(self._predict_with_uncertainty(normalized_spectrum, uncertainty_samples))\n            else:\n                # 单次预测\n                results.update(self._predict_single(normalized_spectrum))\n            \n            if optimize_prediction:\n                # 优化预测结果\n                results.update(self._optimize_prediction(normalized_spectrum, results['predicted_params_norm']))\n        \n        results['prediction_time'] = time.time() - start_time\n        results['input_spectrum'] = target_spectrum\n        \n        print(f\"Structure prediction completed in {results['prediction_time']:.2f}s\")\n        return results\n    \n    def _predict_single(self, normalized_spectrum: torch.Tensor) -> Dict[str, Any]:\n        \"\"\"\n        单次预测\n        \n        Args:\n            normalized_spectrum: 归一化光谱\n            \n        Returns:\n            Dict[str, Any]: 预测结果\n        \"\"\"\n        # 生成器预测\n        predicted_params_norm = self.generator(normalized_spectrum)\n        predicted_params_denorm = denormalize_params(predicted_params_norm, self.dataset.param_ranges)\n        \n        # 前向模型验证\n        reconstructed_spectrum, predicted_metrics_norm = self.forward_model(predicted_params_norm)\n        predicted_metrics_denorm = denormalize_metrics(predicted_metrics_norm, self.dataset.metric_ranges)\n        \n        # 计算重建误差\n        mse_criterion = criterion_mse()\n        reconstruction_mse = mse_criterion(reconstructed_spectrum, normalized_spectrum).item()\n        \n        return {\n            'predicted_params_norm': predicted_params_norm.cpu().numpy(),\n            'predicted_params_denorm': predicted_params_denorm.cpu().numpy(),\n            'reconstructed_spectrum': reconstructed_spectrum.cpu().numpy(),\n            'predicted_metrics_norm': predicted_metrics_norm.cpu().numpy(),\n            'predicted_metrics_denorm': predicted_metrics_denorm.cpu().numpy(),\n            'reconstruction_mse': reconstruction_mse,\n            'uncertainty_available': False\n        }\n    \n    def _predict_with_uncertainty(self, normalized_spectrum: torch.Tensor, \n                                num_samples: int) -> Dict[str, Any]:\n        \"\"\"\n        带不确定性的预测\n        \n        Args:\n            normalized_spectrum: 归一化光谱\n            num_samples: 采样次数\n            \n        Returns:\n            Dict[str, Any]: 预测结果（包含不确定性）\n        \"\"\"\n        # 启用dropout进行Monte Carlo采样\n        self.generator.train()\n        self.forward_model.train()\n        \n        param_samples = []\n        spectrum_samples = []\n        metrics_samples = []\n        \n        for _ in range(num_samples):\n            predicted_params_norm = self.generator(normalized_spectrum)\n            reconstructed_spectrum, predicted_metrics_norm = self.forward_model(predicted_params_norm)\n            \n            param_samples.append(predicted_params_norm.cpu().numpy())\n            spectrum_samples.append(reconstructed_spectrum.cpu().numpy())\n            metrics_samples.append(predicted_metrics_norm.cpu().numpy())\n        \n        # 恢复评估模式\n        self.generator.eval()\n        self.forward_model.eval()\n        \n        # 统计分析\n        param_samples = np.array(param_samples)\n        spectrum_samples = np.array(spectrum_samples)\n        metrics_samples = np.array(metrics_samples)\n        \n        # 计算均值和标准差\n        param_mean = np.mean(param_samples, axis=0)\n        param_std = np.std(param_samples, axis=0)\n        \n        spectrum_mean = np.mean(spectrum_samples, axis=0)\n        spectrum_std = np.std(spectrum_samples, axis=0)\n        \n        metrics_mean = np.mean(metrics_samples, axis=0)\n        metrics_std = np.std(metrics_samples, axis=0)\n        \n        # 反归一化\n        param_mean_denorm = denormalize_params(torch.tensor(param_mean), self.dataset.param_ranges).numpy()\n        metrics_mean_denorm = denormalize_metrics(torch.tensor(metrics_mean), self.dataset.metric_ranges).numpy()\n        \n        # 计算重建误差\n        mse_criterion = criterion_mse()\n        reconstruction_mse = mse_criterion(torch.tensor(spectrum_mean), normalized_spectrum).item()\n        \n        return {\n            'predicted_params_norm': param_mean,\n            'predicted_params_denorm': param_mean_denorm,\n            'predicted_params_std': param_std,\n            'reconstructed_spectrum': spectrum_mean,\n            'reconstructed_spectrum_std': spectrum_std,\n            'predicted_metrics_norm': metrics_mean,\n            'predicted_metrics_denorm': metrics_mean_denorm,\n            'predicted_metrics_std': metrics_std,\n            'reconstruction_mse': reconstruction_mse,\n            'uncertainty_available': True,\n            'num_uncertainty_samples': num_samples,\n            'param_samples': param_samples,\n            'spectrum_samples': spectrum_samples,\n            'metrics_samples': metrics_samples\n        }\n    \n    def _optimize_prediction(self, normalized_spectrum: torch.Tensor, \n                           initial_params: np.ndarray) -> Dict[str, Any]:\n        \"\"\"\n        优化预测结果\n        \n        Args:\n            normalized_spectrum: 归一化光谱\n            initial_params: 初始参数预测\n            \n        Returns:\n            Dict[str, Any]: 优化结果\n        \"\"\"\n        print(\"Optimizing prediction...\")\n        \n        # 将初始参数转换为可优化张量\n        optimized_params = torch.tensor(initial_params, dtype=torch.float32, \n                                       requires_grad=True, device=self.device)\n        \n        # 优化器\n        optimizer = torch.optim.Adam([optimized_params], lr=0.01)\n        mse_criterion = criterion_mse()\n        \n        best_loss = float('inf')\n        best_params = optimized_params.clone()\n        \n        # 优化循环\n        for iteration in range(100):\n            optimizer.zero_grad()\n            \n            # 确保参数在有效范围内\n            clamped_params = torch.clamp(optimized_params, 0, 1)\n            \n            # 前向传播\n            pred_spectrum, _ = self.forward_model(clamped_params)\n            \n            # 计算损失\n            loss = mse_criterion(pred_spectrum, normalized_spectrum)\n            \n            # 反向传播\n            loss.backward()\n            optimizer.step()\n            \n            # 记录最佳结果\n            if loss.item() < best_loss:\n                best_loss = loss.item()\n                best_params = clamped_params.clone()\n            \n            # 早停\n            if loss.item() < 1e-6:\n                break\n        \n        # 最终预测\n        with torch.no_grad():\n            final_spectrum, final_metrics = self.forward_model(best_params)\n            final_params_denorm = denormalize_params(best_params, self.dataset.param_ranges)\n            final_metrics_denorm = denormalize_metrics(final_metrics, self.dataset.metric_ranges)\n        \n        return {\n            'optimized_params_norm': best_params.cpu().numpy(),\n            'optimized_params_denorm': final_params_denorm.cpu().numpy(),\n            'optimized_spectrum': final_spectrum.cpu().numpy(),\n            'optimized_metrics_denorm': final_metrics_denorm.cpu().numpy(),\n            'optimization_loss': best_loss,\n            'optimization_iterations': iteration + 1\n        }\n    \n    def save_prediction_results(self, results: Dict[str, Any], \n                              save_path: str = None) -> None:\n        \"\"\"\n        保存预测结果\n        \n        Args:\n            results: 预测结果\n            save_path: 保存路径\n        \"\"\"\n        if save_path is None:\n            save_path = os.path.join(cfg.PLOTS_DIR, \"prediction_results.npz\")\n        \n        # 准备保存数据\n        save_data = {}\n        for key, value in results.items():\n            if isinstance(value, np.ndarray):\n                save_data[key] = value\n            elif isinstance(value, (int, float, bool)):\n                save_data[key] = value\n        \n        np.savez(save_path, **save_data)\n        print(f\"Prediction results saved to: {save_path}\")\n    \n    def visualize_prediction(self, results: Dict[str, Any], \n                           save_path: str = None) -> None:\n        \"\"\"\n        可视化预测结果\n        \n        Args:\n            results: 预测结果\n            save_path: 保存路径\n        \"\"\"\n        if save_path is None:\n            save_path = os.path.join(cfg.PLOTS_DIR, \"prediction_visualization.png\")\n        \n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        \n        # 光谱重建对比\n        ax1 = axes[0, 0]\n        frequencies = self.dataset.frequencies if hasattr(self.dataset, 'frequencies') else np.arange(len(results['input_spectrum'][0]))\n        \n        ax1.plot(frequencies, results['input_spectrum'][0], 'b-', label='Input Spectrum', linewidth=2)\n        ax1.plot(frequencies, results['reconstructed_spectrum'][0], 'r--', label='Reconstructed', linewidth=2)\n        \n        if results.get('uncertainty_available', False):\n            std = results['reconstructed_spectrum_std'][0]\n            mean = results['reconstructed_spectrum'][0]\n            ax1.fill_between(frequencies, mean - std, mean + std, alpha=0.3, color='red')\n        \n        ax1.set_xlabel('Frequency')\n        ax1.set_ylabel('Spectrum Value')\n        ax1.set_title('Spectrum Reconstruction')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # 预测参数\n        ax2 = axes[0, 1]\n        param_names = getattr(self.dataset, 'param_names', [f'Param_{i}' for i in range(len(results['predicted_params_denorm'][0]))])\n        param_values = results['predicted_params_denorm'][0]\n        \n        bars = ax2.bar(param_names, param_values, color='skyblue', alpha=0.7)\n        \n        if results.get('uncertainty_available', False):\n            param_std_denorm = np.std(denormalize_params(torch.tensor(results['param_samples']), self.dataset.param_ranges).numpy(), axis=0)[0]\n            ax2.errorbar(param_names, param_values, yerr=param_std_denorm, fmt='none', color='black', capsize=5)\n        \n        ax2.set_ylabel('Parameter Value')\n        ax2.set_title('Predicted Parameters')\n        ax2.tick_params(axis='x', rotation=45)\n        \n        # 预测指标\n        ax3 = axes[1, 0]\n        metric_names = getattr(self.dataset, 'metric_names', [f'Metric_{i}' for i in range(len(results['predicted_metrics_denorm'][0]))])\n        metric_values = results['predicted_metrics_denorm'][0]\n        \n        bars = ax3.bar(metric_names, metric_values, color='lightgreen', alpha=0.7)\n        \n        if results.get('uncertainty_available', False):\n            metric_std_denorm = np.std(denormalize_metrics(torch.tensor(results['metrics_samples']), self.dataset.metric_ranges).numpy(), axis=0)[0]\n            ax3.errorbar(metric_names, metric_values, yerr=metric_std_denorm, fmt='none', color='black', capsize=5)\n        \n        ax3.set_ylabel('Metric Value')\n        ax3.set_title('Predicted Metrics')\n        ax3.tick_params(axis='x', rotation=45)\n        \n        # 误差分析\n        ax4 = axes[1, 1]\n        error_info = [\n            ('Reconstruction MSE', results['reconstruction_mse']),\n            ('Prediction Time (s)', results['prediction_time'])\n        ]\n        \n        if results.get('optimization_loss'):\n            error_info.append(('Optimization Loss', results['optimization_loss']))\n        \n        labels, values = zip(*error_info)\n        ax4.barh(labels, values, color='orange', alpha=0.7)\n        ax4.set_xlabel('Value')\n        ax4.set_title('Performance Metrics')\n        \n        plt.tight_layout()\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        print(f\"Prediction visualization saved to: {save_path}\")\n\n\ndef main():\n    \"\"\"\n    主函数：命令行接口\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Enhanced Structure Prediction\")\n    parser.add_argument('--input', type=str, required=True,\n                        help='Input spectrum file path (.txt, .npy, .csv)')\n    parser.add_argument('--model_dir', type=str, default=None,\n                        help='Model directory path')\n    parser.add_argument('--uncertainty_samples', type=int, default=0,\n                        help='Number of uncertainty samples (0 for no uncertainty estimation)')\n    parser.add_argument('--optimize', action='store_true',\n                        help='Enable prediction optimization')\n    parser.add_argument('--output_dir', type=str, default=None,\n                        help='Output directory for results')\n    \n    args = parser.parse_args()\n    \n    # 设置随机种子\n    set_seed(cfg.RANDOM_SEED)\n    \n    # 创建预测器\n    predictor = EnhancedPredictor()\n    \n    # 加载模型和数据集\n    if not predictor.load_models(args.model_dir):\n        print(\"Failed to load models!\")\n        return\n    \n    if not predictor.load_dataset():\n        print(\"Failed to load dataset!\")\n        return\n    \n    # 执行预测\n    try:\n        results = predictor.predict_structure(\n            spectrum_input=args.input,\n            uncertainty_samples=args.uncertainty_samples,\n            optimize_prediction=args.optimize\n        )\n        \n        # 打印结果\n        print(\"\\n=== PREDICTION RESULTS ===\")\n        print(f\"Predicted Parameters: {results['predicted_params_denorm'][0]}\")\n        print(f\"Predicted Metrics: {results['predicted_metrics_denorm'][0]}\")\n        print(f\"Reconstruction MSE: {results['reconstruction_mse']:.6f}\")\n        \n        if results.get('uncertainty_available'):\n            print(f\"Uncertainty estimation with {results['num_uncertainty_samples']} samples\")\n        \n        if results.get('optimization_loss'):\n            print(f\"Optimization completed in {results['optimization_iterations']} iterations\")\n            print(f\"Final optimization loss: {results['optimization_loss']:.6f}\")\n        \n        # 保存结果\n        if args.output_dir:\n            os.makedirs(args.output_dir, exist_ok=True)\n            predictor.save_prediction_results(results, \n                                            os.path.join(args.output_dir, \"prediction_results.npz\"))\n            predictor.visualize_prediction(results, \n                                         os.path.join(args.output_dir, \"prediction_visualization.png\"))\n        else:\n            predictor.save_prediction_results(results)\n            predictor.visualize_prediction(results)\n        \n        print(\"\\nPrediction completed successfully!\")\n        \n    except Exception as e:\n        print(f\"Prediction failed: {e}\")\n        import traceback\n        traceback.print_exc()\n\n\nif __name__ == \"__main__\":\n    main()