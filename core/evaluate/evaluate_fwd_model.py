# PI_GAN_THZ/core/evaluate/evaluate_fwd_model.py

import sys
import os
import torch
import numpy as np
import argparse

# Â∞ÜÈ°πÁõÆÊ†πÁõÆÂΩïÊ∑ªÂä†Âà∞ Python Ë∑ØÂæÑ
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

# ÂØºÂÖ•Áªü‰∏ÄËØÑ‰º∞Âô®
from core.evaluate.unified_evaluator import UnifiedEvaluator
import config.config as cfg
from core.utils.set_seed import set_seed

def evaluate_forward_model(model_dir: str = None, 
                          data_path: str = None, 
                          num_samples: int = 1000):
    """
    ËØÑ‰º∞ÂâçÂêëÊ®°ÂûãÊÄßËÉΩ
    
    Args:
        model_dir: Ê®°ÂûãÁõÆÂΩïË∑ØÂæÑ
        data_path: Êï∞ÊçÆÈõÜË∑ØÂæÑ
        num_samples: ËØÑ‰º∞Ê†∑Êú¨Êï∞
    """
    print("\n--- Forward Model Evaluation ---")
    
    # ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê
    set_seed(cfg.RANDOM_SEED)
    
    # ÂàõÂª∫ËØÑ‰º∞Âô®
    evaluator = UnifiedEvaluator()
    
    # Âä†ËΩΩÊ®°ÂûãÂíåÊï∞ÊçÆ
    if not evaluator.load_models(model_dir):
        print("‚ùå Failed to load models!")
        return
    
    if not evaluator.load_dataset(data_path):
        print("‚ùå Failed to load dataset!")
        return
    
    # ËøêË°åÂâçÂêëÁΩëÁªúËØÑ‰º∞
    results = evaluator.evaluate_forward_network(num_samples)
    
    # ÊâìÂç∞ËØ¶ÁªÜÁªìÊûú
    print("\nüìä Forward Model Evaluation Results:")
    print("-" * 50)
    
    spectrum_metrics = results['spectrum_prediction']
    print("Spectrum Prediction:")
    print(f"  - R¬≤: {spectrum_metrics['r2']:.6f}")
    print(f"  - MAE: {spectrum_metrics['mae']:.6f}")
    print(f"  - RMSE: {spectrum_metrics['rmse']:.6f}")
    print(f"  - Pearson R: {spectrum_metrics['pearson_r']:.6f}")
    print(f"  - MAPE: {spectrum_metrics['mape']:.2f}%")
    
    metrics_metrics = results['metrics_prediction']
    print("\nMetrics Prediction:")
    print(f"  - R¬≤: {metrics_metrics['r2']:.6f}")
    print(f"  - MAE: {metrics_metrics['mae']:.6f}")
    print(f"  - RMSE: {metrics_metrics['rmse']:.6f}")
    print(f"  - Pearson R: {metrics_metrics['pearson_r']:.6f}")
    print(f"  - MAPE: {metrics_metrics['mape']:.2f}%")
    
    # ÊÄßËÉΩËØÑ‰º∞
    print("\nüéØ Performance Assessment:")
    if spectrum_metrics['r2'] > 0.9 and metrics_metrics['r2'] > 0.9:
        print("‚úÖ Forward model shows EXCELLENT performance!")
    elif spectrum_metrics['r2'] > 0.8 and metrics_metrics['r2'] > 0.8:
        print("‚úÖ Forward model shows GOOD performance!")
    elif spectrum_metrics['r2'] > 0.6 and metrics_metrics['r2'] > 0.6:
        print("‚ö†Ô∏è Forward model shows MODERATE performance.")
    else:
        print("‚ùå Forward model shows POOR performance and needs improvement.")
    
    print(f"\n‚úÖ Forward model evaluation completed with {results['num_samples']} samples.")

def main():
    parser = argparse.ArgumentParser(description="Evaluate forward model performance")
    parser.add_argument('--model_dir', type=str, default=None,
                        help='Directory containing trained models')
    parser.add_argument('--data_path', type=str, default=None,
                        help='Path to dataset CSV file')
    parser.add_argument('--num_samples', type=int, default=1000,
                        help='Number of samples for evaluation')
    
    args = parser.parse_args()
    
    evaluate_forward_model(
        model_dir=args.model_dir,
        data_path=args.data_path,
        num_samples=args.num_samples
    )

if __name__ == "__main__":
    main()