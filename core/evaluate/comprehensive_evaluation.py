# PI_GAN_THZ/core/evaluate/comprehensive_evaluation.py

import sys
import os
import argparse
import time

# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

from enhanced_evaluator import EnhancedEvaluator
from enhanced_predict import EnhancedPredictor
import config.config as cfg
from core.utils.set_seed import set_seed

def run_comprehensive_evaluation(model_dir: str = None, 
                                data_path: str = None,
                                num_samples: int = 1000,
                                save_results: bool = True,
                                generate_report: bool = True) -> None:
    """
    è¿è¡Œå…¨é¢çš„æ¨¡å‹è¯„ä¼°
    
    Args:
        model_dir: æ¨¡å‹ç›®å½•
        data_path: æ•°æ®è·¯å¾„
        num_samples: è¯„ä¼°æ ·æœ¬æ•°
        save_results: æ˜¯å¦ä¿å­˜ç»“æœ
        generate_report: æ˜¯å¦ç”ŸæˆæŠ¥å‘Š
    """
    print("\\n" + "="*80)
    print("PI-GAN COMPREHENSIVE EVALUATION")
    print("="*80)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(cfg.RANDOM_SEED)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = EnhancedEvaluator()
    
    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    print("\\n1. Loading models and dataset...")
    if not evaluator.load_models(model_dir):
        print("âŒ Failed to load models!")
        return
    
    if not evaluator.load_dataset(data_path):
        print("âŒ Failed to load dataset!")
        return
    
    print("âœ… Models and dataset loaded successfully!")
    
    # æ‰§è¡Œå…¨é¢è¯„ä¼°
    print(f"\\n2. Running comprehensive evaluation with {num_samples} samples...")
    try:
        results = evaluator.comprehensive_evaluation(num_samples)
        print("âœ… Comprehensive evaluation completed!")
        
        # æ‰“å°å…³é”®ç»“æœ
        print_evaluation_summary(results)
        
        # ä¿å­˜ç»“æœ
        if save_results:
            print("\\n3. Saving evaluation results...")
            evaluator.save_evaluation_results()
            print("âœ… Results saved!")
        
        # ç”ŸæˆæŠ¥å‘Š
        if generate_report:
            print("\\n4. Generating evaluation report...")
            report = evaluator.generate_report()
            print("âœ… Report generated!")
            
            # æ‰“å°æŠ¥å‘Šæ‘˜è¦
            print("\\n" + "="*50)
            print("EVALUATION REPORT SUMMARY")
            print("="*50)
            print(report[-1000:])  # æ‰“å°æŠ¥å‘Šæœ«å°¾çš„æ€»ç»“éƒ¨åˆ†
        
        print("\\nğŸ‰ Comprehensive evaluation completed successfully!")
        
    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        import traceback
        traceback.print_exc()

def print_evaluation_summary(results: dict) -> None:
    """
    æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦
    
    Args:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print("\\n" + "="*50)
    print("EVALUATION SUMMARY")
    print("="*50)
    
    # ç”Ÿæˆå™¨è¯„ä¼°
    gen_results = results['generator_evaluation']
    print("\\nğŸ“Š Generator Performance:")
    print(f"  Parameter Prediction RÂ²: {gen_results['parameter_prediction']['r2']:.4f}")
    print(f"  Parameter Prediction RMSE: {gen_results['parameter_prediction']['rmse']:.6f}")
    print(f"  Spectrum Reconstruction RÂ²: {gen_results['spectrum_reconstruction']['r2']:.4f}")
    print(f"  Metrics Prediction RÂ²: {gen_results['metrics_prediction']['r2']:.4f}")
    
    # åˆ¤åˆ«å™¨è¯„ä¼°
    disc_results = results['discriminator_evaluation']
    print("\\nğŸ¯ Discriminator Performance:")
    print(f"  Overall Accuracy: {disc_results['overall_accuracy']:.4f}")
    print(f"  Real Sample Accuracy: {disc_results['real_accuracy']:.4f}")
    print(f"  Fake Sample Accuracy: {disc_results['fake_accuracy']:.4f}")
    
    # ç‰©ç†ä¸€è‡´æ€§
    phys_results = results['physics_consistency']
    print("\\nâš—ï¸ Physics Consistency:")
    print(f"  Parameter Range Violation Rate: {phys_results['param_range_violation_rate']:.4f}")
    print(f"  Average Violations per Sample: {phys_results['avg_param_violations_per_sample']:.4f}")
    
    print(f"\\nâ±ï¸ Total Evaluation Time: {results['evaluation_time']:.2f}s")
    print(f"ğŸ“ˆ Number of Samples Evaluated: {results['num_samples']}")

def run_prediction_test(model_dir: str = None,
                       test_spectrum_path: str = None,
                       uncertainty_samples: int = 100) -> None:
    """
    è¿è¡Œé¢„æµ‹æµ‹è¯•
    
    Args:
        model_dir: æ¨¡å‹ç›®å½•
        test_spectrum_path: æµ‹è¯•å…‰è°±è·¯å¾„
        uncertainty_samples: ä¸ç¡®å®šæ€§é‡‡æ ·æ¬¡æ•°
    """
    print("\\n" + "="*50)
    print("PREDICTION TEST")
    print("="*50)
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = EnhancedPredictor()
    
    # åŠ è½½æ¨¡å‹
    if not predictor.load_models(model_dir):
        print("âŒ Failed to load models for prediction!")
        return
    
    if not predictor.load_dataset():
        print("âŒ Failed to load dataset for prediction!")
        return
    
    # æµ‹è¯•é¢„æµ‹
    if test_spectrum_path and os.path.exists(test_spectrum_path):
        print(f"\\nğŸ”¬ Testing prediction with: {test_spectrum_path}")
        try:
            results = predictor.predict_structure(
                spectrum_input=test_spectrum_path,
                uncertainty_samples=uncertainty_samples,
                optimize_prediction=True
            )
            
            print("\\nâœ… Prediction completed!")
            print(f"Predicted Parameters: {results['predicted_params_denorm'][0]}")
            print(f"Reconstruction MSE: {results['reconstruction_mse']:.6f}")
            print(f"Prediction Time: {results['prediction_time']:.2f}s")
            
            if results.get('uncertainty_available'):
                print(f"Uncertainty estimation completed with {results['num_uncertainty_samples']} samples")
            
            # å¯è§†åŒ–ç»“æœ
            predictor.visualize_prediction(results)
            predictor.save_prediction_results(results)
            
        except Exception as e:
            print(f"âŒ Prediction test failed: {e}")
    else:
        print(f"âš ï¸ Test spectrum file not found: {test_spectrum_path}")

def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description="PI-GAN Comprehensive Evaluation")
    parser.add_argument('--model_dir', type=str, default=None,
                        help='Model directory path')
    parser.add_argument('--data_path', type=str, default=None,
                        help='Dataset path')
    parser.add_argument('--num_samples', type=int, default=1000,
                        help='Number of samples for evaluation')
    parser.add_argument('--test_spectrum', type=str, default=None,
                        help='Path to test spectrum file for prediction test')
    parser.add_argument('--uncertainty_samples', type=int, default=100,
                        help='Number of uncertainty samples for prediction test')
    parser.add_argument('--skip_evaluation', action='store_true',
                        help='Skip comprehensive evaluation')
    parser.add_argument('--skip_prediction', action='store_true',
                        help='Skip prediction test')
    parser.add_argument('--no_save', action='store_true',
                        help='Do not save results')
    parser.add_argument('--no_report', action='store_true',
                        help='Do not generate report')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    cfg.create_directories()
    
    start_time = time.time()
    
    # è¿è¡Œå…¨é¢è¯„ä¼°
    if not args.skip_evaluation:
        run_comprehensive_evaluation(
            model_dir=args.model_dir,
            data_path=args.data_path,
            num_samples=args.num_samples,
            save_results=not args.no_save,
            generate_report=not args.no_report
        )
    
    # è¿è¡Œé¢„æµ‹æµ‹è¯•
    if not args.skip_prediction:
        # å¦‚æœæ²¡æœ‰æä¾›æµ‹è¯•å…‰è°±ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„
        test_spectrum = args.test_spectrum
        if test_spectrum is None:
            # å°è¯•ä¸€äº›é»˜è®¤è·¯å¾„
            possible_paths = [
                os.path.join(cfg.DATA_DIR, "THZ.txt"),
                os.path.join(cfg.PROJECT_ROOT, "dataset", "THZ.txt"),
                os.path.join(cfg.PROJECT_ROOT, "test_spectrum.txt")
            ]
            for path in possible_paths:
                if os.path.exists(path):
                    test_spectrum = path
                    break
        
        if test_spectrum:
            run_prediction_test(
                model_dir=args.model_dir,
                test_spectrum_path=test_spectrum,
                uncertainty_samples=args.uncertainty_samples
            )
        else:
            print("\\nâš ï¸ No test spectrum provided, skipping prediction test")
            print("   Use --test_spectrum to specify a test file")
    
    total_time = time.time() - start_time
    print(f"\\nğŸ Total evaluation time: {total_time:.2f}s")
    print("\\nâœ¨ All evaluations completed!")

if __name__ == "__main__":
    main()