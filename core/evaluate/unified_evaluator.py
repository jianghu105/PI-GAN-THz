# PI_GAN_THZ/core/evaluate/unified_evaluator.py

import sys
import os
import torch
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from torch.utils.data import DataLoader, Subset
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
import time
import argparse

# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

from core.models.generator import Generator
from core.models.discriminator import Discriminator
from core.models.forward_model import ForwardModel
import config.config as cfg
from core.utils.data_loader import MetamaterialDataset, denormalize_params, denormalize_metrics, normalize_spectrum
from core.utils.set_seed import set_seed
from core.utils.loss import criterion_mse, criterion_bce
from core.utils.visualization import EvaluationVisualizer

class UnifiedEvaluator:
    """
    ç»Ÿä¸€è¯„ä¼°å™¨ï¼šåŒ…å«å‰å‘ç½‘ç»œè¯„ä¼°ã€PI-GANè¯„ä¼°ã€ç»“æ„é¢„æµ‹å’Œæ¨¡å‹éªŒè¯
    """
    
    def __init__(self, device: str = "auto"):
        """
        åˆå§‹åŒ–ç»Ÿä¸€è¯„ä¼°å™¨
        
        Args:
            device: è®¡ç®—è®¾å¤‡ ("auto", "cpu", "cuda")
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu") if device == "auto" else torch.device(device)
        self.generator = None
        self.discriminator = None
        self.forward_model = None
        self.dataset = None
        self.evaluation_results = {}
        
        # åˆå§‹åŒ–å¯è§†åŒ–å™¨
        plots_dir = os.path.join(cfg.PROJECT_ROOT, "plots")
        self.visualizer = EvaluationVisualizer(save_dir=plots_dir)
        
        print(f"Unified Evaluator initialized on device: {self.device}")
    
    def load_models(self, model_dir: str = None) -> bool:
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            
        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        if model_dir is None:
            model_dir = cfg.SAVED_MODELS_DIR
            
        print(f"Loading models from: {model_dir}")
        
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            self.generator = Generator(
                input_dim=cfg.SPECTRUM_DIM, 
                output_dim=cfg.GENERATOR_OUTPUT_PARAM_DIM
            ).to(self.device)
            
            self.discriminator = Discriminator(
                input_spec_dim=cfg.DISCRIMINATOR_INPUT_SPEC_DIM,
                input_param_dim=cfg.DISCRIMINATOR_INPUT_PARAM_DIM
            ).to(self.device)
            
            self.forward_model = ForwardModel(
                input_param_dim=cfg.FORWARD_MODEL_INPUT_DIM,
                output_spectrum_dim=cfg.FORWARD_MODEL_OUTPUT_SPEC_DIM,
                output_metrics_dim=cfg.FORWARD_MODEL_OUTPUT_METRICS_DIM
            ).to(self.device)
            
            # åŠ è½½æƒé‡
            gen_path = os.path.join(model_dir, "generator_final.pth")
            disc_path = os.path.join(model_dir, "discriminator_final.pth")
            fwd_path = os.path.join(model_dir, "forward_model_final.pth")
            
            if not all(os.path.exists(p) for p in [gen_path, disc_path, fwd_path]):
                print("Error: Model files not found!")
                return False
                
            self.generator.load_state_dict(torch.load(gen_path, map_location=self.device))
            self.discriminator.load_state_dict(torch.load(disc_path, map_location=self.device))
            self.forward_model.load_state_dict(torch.load(fwd_path, map_location=self.device))
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.generator.eval()
            self.discriminator.eval()
            self.forward_model.eval()
            
            print("âœ“ Models loaded successfully!")
            return True
            
        except Exception as e:
            print(f"âœ— Error loading models: {e}")
            return False
    
    def load_dataset(self, data_path: str = None) -> bool:
        """
        åŠ è½½æ•°æ®é›†
        
        Args:
            data_path: æ•°æ®é›†è·¯å¾„
            
        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        if data_path is None:
            data_path = cfg.DATASET_PATH
            
        try:
            self.dataset = MetamaterialDataset(
                data_path=data_path, 
                num_points_per_sample=cfg.SPECTRUM_DIM
            )
            print(f"âœ“ Dataset loaded: {len(self.dataset)} samples")
            return True
            
        except Exception as e:
            print(f"âœ— Error loading dataset: {e}")
            return False
    
    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """
        è®¡ç®—å›å½’è¯„ä¼°æŒ‡æ ‡
        
        Args:
            y_true: çœŸå®å€¼
            y_pred: é¢„æµ‹å€¼
            
        Returns:
            Dict[str, float]: è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        
        # åŸºç¡€å›å½’æŒ‡æ ‡
        metrics['mse'] = mean_squared_error(y_true, y_pred)
        metrics['mae'] = mean_absolute_error(y_true, y_pred)
        metrics['rmse'] = np.sqrt(metrics['mse'])
        
        # ç›¸å…³æ€§æŒ‡æ ‡
        try:
            metrics['r2'] = r2_score(y_true, y_pred)
        except:
            metrics['r2'] = float('nan')
            
        # ç›¸å…³ç³»æ•°
        try:
            if y_true.ndim == 1:
                pearson_corr, _ = pearsonr(y_true, y_pred)
                metrics['pearson_r'] = pearson_corr
            else:
                # å¤šç»´æ•°æ®è®¡ç®—å¹³å‡ç›¸å…³ç³»æ•°
                pearson_corrs = []
                for i in range(y_true.shape[1]):
                    try:
                        p_corr, _ = pearsonr(y_true[:, i], y_pred[:, i])
                        pearson_corrs.append(p_corr)
                    except:
                        pass
                        
                metrics['pearson_r'] = np.mean(pearson_corrs) if pearson_corrs else float('nan')
        except:
            metrics['pearson_r'] = float('nan')
        
        # ç›¸å¯¹è¯¯å·®
        metrics['mape'] = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
        
        return metrics
    
    def evaluate_forward_network(self, num_samples: int = 1000) -> Dict[str, Any]:
        """
        è¯„ä¼°å‰å‘ç½‘ç»œæ€§èƒ½
        
        Args:
            num_samples: è¯„ä¼°æ ·æœ¬æ•°
            
        Returns:
            Dict[str, Any]: å‰å‘ç½‘ç»œè¯„ä¼°ç»“æœ
        """
        print(f"\n=== Forward Network Evaluation ({num_samples} samples) ===")
        
        if self.forward_model is None or self.dataset is None:
            raise ValueError("Forward model and dataset must be loaded first!")
        
        # éšæœºé‡‡æ ·
        sample_indices = np.random.choice(len(self.dataset), min(num_samples, len(self.dataset)), replace=False)
        subset = Subset(self.dataset, sample_indices)
        dataloader = DataLoader(subset, batch_size=64, shuffle=False)
        
        all_real_spectra = []
        all_pred_spectra = []
        all_real_metrics = []
        all_pred_metrics = []
        
        with torch.no_grad():
            for batch in dataloader:
                real_spectrum, _, real_params_norm, real_metrics_denorm, real_metrics_norm = batch
                
                real_spectrum = real_spectrum.to(self.device)
                real_params_norm = real_params_norm.to(self.device)
                real_metrics_norm = real_metrics_norm.to(self.device)
                
                # å‰å‘æ¨¡å‹é¢„æµ‹
                pred_spectrum, pred_metrics_norm = self.forward_model(real_params_norm)
                pred_metrics_denorm = denormalize_metrics(pred_metrics_norm, self.dataset.metric_ranges)
                
                # æ”¶é›†ç»“æœ
                all_real_spectra.append(real_spectrum.cpu().numpy())
                all_pred_spectra.append(pred_spectrum.cpu().numpy())
                all_real_metrics.append(real_metrics_denorm.cpu().numpy())
                all_pred_metrics.append(pred_metrics_denorm.cpu().numpy())
        
        # åˆå¹¶ç»“æœ
        all_real_spectra = np.concatenate(all_real_spectra, axis=0)
        all_pred_spectra = np.concatenate(all_pred_spectra, axis=0)
        all_real_metrics = np.concatenate(all_real_metrics, axis=0)
        all_pred_metrics = np.concatenate(all_pred_metrics, axis=0)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        spectrum_metrics = self.calculate_metrics(all_real_spectra, all_pred_spectra)
        metrics_metrics = self.calculate_metrics(all_real_metrics, all_pred_metrics)
        
        results = {
            'spectrum_prediction': spectrum_metrics,
            'metrics_prediction': metrics_metrics,
            'num_samples': len(all_real_spectra),
            'data_samples': {
                'real_spectra': all_real_spectra[:50],  # ä¿å­˜å‰50ä¸ªæ ·æœ¬ç”¨äºå¯è§†åŒ–
                'pred_spectra': all_pred_spectra[:50],
                'real_metrics': all_real_metrics[:50],
                'pred_metrics': all_pred_metrics[:50]
            }
        }
        
        print(f"âœ“ Forward network evaluation completed")
        print(f"  - Spectrum RÂ²: {spectrum_metrics['r2']:.4f}")
        print(f"  - Metrics RÂ²: {metrics_metrics['r2']:.4f}")
        
        return results
    
    def evaluate_pigan(self, num_samples: int = 1000) -> Dict[str, Any]:
        """
        è¯„ä¼°PI-GANæ€§èƒ½ï¼ˆç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ï¼‰
        
        Args:
            num_samples: è¯„ä¼°æ ·æœ¬æ•°
            
        Returns:
            Dict[str, Any]: PI-GANè¯„ä¼°ç»“æœ
        """
        print(f"\n=== PI-GAN Evaluation ({num_samples} samples) ===")
        
        if not all([self.generator, self.discriminator, self.forward_model, self.dataset]):
            raise ValueError("All models and dataset must be loaded first!")
        
        # éšæœºé‡‡æ ·
        sample_indices = np.random.choice(len(self.dataset), min(num_samples, len(self.dataset)), replace=False)
        subset = Subset(self.dataset, sample_indices)
        dataloader = DataLoader(subset, batch_size=64, shuffle=False)
        
        # ç”Ÿæˆå™¨è¯„ä¼°
        all_real_params = []
        all_pred_params = []
        all_real_scores = []
        all_fake_scores = []
        
        with torch.no_grad():
            for batch in dataloader:
                real_spectrum, real_params_denorm, real_params_norm, _, _ = batch
                
                real_spectrum = real_spectrum.to(self.device)
                real_params_denorm = real_params_denorm.to(self.device)
                real_params_norm = real_params_norm.to(self.device)
                
                # ç”Ÿæˆå™¨é¢„æµ‹å‚æ•°
                pred_params_norm = self.generator(real_spectrum)
                pred_params_denorm = denormalize_params(pred_params_norm, self.dataset.param_ranges)
                
                # åˆ¤åˆ«å™¨è¯„åˆ†
                real_scores = self.discriminator(real_spectrum, real_params_denorm)
                fake_scores = self.discriminator(real_spectrum, pred_params_denorm)
                
                # æ”¶é›†ç»“æœ
                all_real_params.append(real_params_denorm.cpu().numpy())
                all_pred_params.append(pred_params_denorm.cpu().numpy())
                all_real_scores.append(real_scores.cpu().numpy())
                all_fake_scores.append(fake_scores.cpu().numpy())
        
        # åˆå¹¶ç»“æœ
        all_real_params = np.concatenate(all_real_params, axis=0)
        all_pred_params = np.concatenate(all_pred_params, axis=0)
        all_real_scores = np.concatenate(all_real_scores, axis=0)
        all_fake_scores = np.concatenate(all_fake_scores, axis=0)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        param_metrics = self.calculate_metrics(all_real_params, all_pred_params)
        
        # åˆ¤åˆ«å™¨æ€§èƒ½
        real_accuracy = np.mean(all_real_scores > 0.5)
        fake_accuracy = np.mean(all_fake_scores < 0.5)
        overall_accuracy = (real_accuracy + fake_accuracy) / 2
        
        results = {
            'parameter_prediction': param_metrics,
            'discriminator_performance': {
                'real_accuracy': real_accuracy,
                'fake_accuracy': fake_accuracy,
                'overall_accuracy': overall_accuracy,
                'real_score_mean': np.mean(all_real_scores),
                'fake_score_mean': np.mean(all_fake_scores)
            },
            'num_samples': len(all_real_params),
            'data_samples': {
                'real_params': all_real_params[:50],  # ä¿å­˜å‰50ä¸ªæ ·æœ¬ç”¨äºå¯è§†åŒ–
                'pred_params': all_pred_params[:50]
            },
            'score_distributions': {
                'real_scores': all_real_scores[:200],  # ä¿å­˜å‰200ä¸ªå¾—åˆ†ç”¨äºå¯è§†åŒ–
                'fake_scores': all_fake_scores[:200]
            }
        }
        
        print(f"âœ“ PI-GAN evaluation completed")
        print(f"  - Parameter RÂ²: {param_metrics['r2']:.4f}")
        print(f"  - Discriminator Accuracy: {overall_accuracy:.4f}")
        
        return results
    
    def evaluate_structural_prediction(self, num_samples: int = 100) -> Dict[str, Any]:
        """
        è¯„ä¼°ç»“æ„é¢„æµ‹èƒ½åŠ›
        
        Args:
            num_samples: è¯„ä¼°æ ·æœ¬æ•°
            
        Returns:
            Dict[str, Any]: ç»“æ„é¢„æµ‹è¯„ä¼°ç»“æœ
        """
        print(f"\n=== Structural Prediction Evaluation ({num_samples} samples) ===")
        
        if not all([self.generator, self.forward_model, self.dataset]):
            raise ValueError("Generator, forward model and dataset must be loaded first!")
        
        # éšæœºé‡‡æ ·
        sample_indices = np.random.choice(len(self.dataset), min(num_samples, len(self.dataset)), replace=False)
        subset = Subset(self.dataset, sample_indices)
        dataloader = DataLoader(subset, batch_size=32, shuffle=False)
        
        param_range_violations = []
        reconstruction_errors = []
        consistency_scores = []
        
        with torch.no_grad():
            for batch in dataloader:
                real_spectrum, real_params_denorm, real_params_norm, _, _ = batch
                
                real_spectrum = real_spectrum.to(self.device)
                real_params_norm = real_params_norm.to(self.device)
                
                # ç”Ÿæˆå™¨é¢„æµ‹å‚æ•°
                pred_params_norm = self.generator(real_spectrum)
                
                # æ£€æŸ¥å‚æ•°èŒƒå›´çº¦æŸ
                range_violations = torch.sum((pred_params_norm < 0) | (pred_params_norm > 1), dim=1).cpu().numpy()
                param_range_violations.extend(range_violations)
                
                # å‰å‘æ¨¡å‹é‡å»ºå…‰è°±
                recon_spectrum, _ = self.forward_model(pred_params_norm)
                
                # é‡å»ºè¯¯å·®
                recon_error = torch.mean((real_spectrum - recon_spectrum) ** 2, dim=1).cpu().numpy()
                reconstruction_errors.extend(recon_error)
                
                # ä¸€è‡´æ€§å¾—åˆ† (1 - å½’ä¸€åŒ–é‡å»ºè¯¯å·®)
                consistency = 1.0 / (1.0 + recon_error)
                consistency_scores.extend(consistency)
        
        # ç»Ÿè®¡ç»“æœ
        param_range_violations = np.array(param_range_violations)
        reconstruction_errors = np.array(reconstruction_errors)
        consistency_scores = np.array(consistency_scores)
        
        results = {
            'param_range_violation_rate': np.mean(param_range_violations > 0),
            'avg_param_violations': np.mean(param_range_violations),
            'reconstruction_error_mean': np.mean(reconstruction_errors),
            'reconstruction_error_std': np.std(reconstruction_errors),
            'consistency_score_mean': np.mean(consistency_scores),
            'consistency_score_std': np.std(consistency_scores),
            'num_samples': len(param_range_violations)
        }
        
        print(f"âœ“ Structural prediction evaluation completed")
        print(f"  - Parameter violation rate: {results['param_range_violation_rate']:.4f}")
        print(f"  - Consistency score: {results['consistency_score_mean']:.4f}")
        
        return results
    
    def evaluate_model_validation(self, num_samples: int = 500) -> Dict[str, Any]:
        """
        æ¨¡å‹éªŒè¯è¯„ä¼°
        
        Args:
            num_samples: è¯„ä¼°æ ·æœ¬æ•°
            
        Returns:
            Dict[str, Any]: æ¨¡å‹éªŒè¯ç»“æœ
        """
        print(f"\n=== Model Validation ({num_samples} samples) ===")
        
        if not all([self.generator, self.forward_model, self.dataset]):
            raise ValueError("Generator, forward model and dataset must be loaded first!")
        
        # éšæœºé‡‡æ ·
        sample_indices = np.random.choice(len(self.dataset), min(num_samples, len(self.dataset)), replace=False)
        subset = Subset(self.dataset, sample_indices)
        dataloader = DataLoader(subset, batch_size=64, shuffle=False)
        
        cycle_consistency_errors = []
        prediction_stability = []
        physical_plausibility = []
        
        with torch.no_grad():
            for batch in dataloader:
                real_spectrum, real_params_denorm, real_params_norm, _, _ = batch
                
                real_spectrum = real_spectrum.to(self.device)
                real_params_norm = real_params_norm.to(self.device)
                
                # å¾ªç¯ä¸€è‡´æ€§æµ‹è¯•: spectrum -> params -> spectrum
                pred_params_norm = self.generator(real_spectrum)
                recon_spectrum, _ = self.forward_model(pred_params_norm)
                
                cycle_error = torch.mean((real_spectrum - recon_spectrum) ** 2, dim=1).cpu().numpy()
                cycle_consistency_errors.extend(cycle_error)
                
                # é¢„æµ‹ç¨³å®šæ€§æµ‹è¯•: æ·»åŠ å°å™ªå£°åçš„é¢„æµ‹ä¸€è‡´æ€§
                noise = torch.randn_like(real_spectrum) * 0.01
                noisy_spectrum = real_spectrum + noise
                pred_params_noisy = self.generator(noisy_spectrum)
                
                stability = torch.mean((pred_params_norm - pred_params_noisy) ** 2, dim=1).cpu().numpy()
                prediction_stability.extend(stability)
                
                # ç‰©ç†åˆç†æ€§: é¢„æµ‹å‚æ•°çš„ç‰©ç†çº¦æŸæ»¡è¶³ç¨‹åº¦
                pred_params_denorm = denormalize_params(pred_params_norm, self.dataset.param_ranges)
                
                # æ£€æŸ¥å‚æ•°æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
                plausibility_score = torch.mean(
                    torch.sigmoid(pred_params_norm * 10 - 5), dim=1
                ).cpu().numpy()
                physical_plausibility.extend(plausibility_score)
        
        # ç»Ÿè®¡ç»“æœ
        cycle_consistency_errors = np.array(cycle_consistency_errors)
        prediction_stability = np.array(prediction_stability)
        physical_plausibility = np.array(physical_plausibility)
        
        results = {
            'cycle_consistency_error_mean': np.mean(cycle_consistency_errors),
            'cycle_consistency_error_std': np.std(cycle_consistency_errors),
            'prediction_stability_mean': np.mean(prediction_stability),
            'prediction_stability_std': np.std(prediction_stability),
            'physical_plausibility_mean': np.mean(physical_plausibility),
            'physical_plausibility_std': np.std(physical_plausibility),
            'num_samples': len(cycle_consistency_errors)
        }
        
        print(f"âœ“ Model validation completed")
        print(f"  - Cycle consistency error: {results['cycle_consistency_error_mean']:.6f}")
        print(f"  - Prediction stability: {results['prediction_stability_mean']:.6f}")
        print(f"  - Physical plausibility: {results['physical_plausibility_mean']:.4f}")
        
        return results
    
    def run_comprehensive_evaluation(self, num_samples: int = 1000) -> Dict[str, Any]:
        """
        è¿è¡Œå…¨é¢è¯„ä¼°
        
        Args:
            num_samples: è¯„ä¼°æ ·æœ¬æ•°
            
        Returns:
            Dict[str, Any]: å®Œæ•´è¯„ä¼°ç»“æœ
        """
        print("\n" + "="*80)
        print("PI-GAN COMPREHENSIVE EVALUATION")
        print("="*80)
        
        start_time = time.time()
        
        # æ£€æŸ¥æ¨¡å‹å’Œæ•°æ®é›†
        if not all([self.generator, self.discriminator, self.forward_model, self.dataset]):
            raise ValueError("All models and dataset must be loaded first!")
        
        # æ‰§è¡Œå„é¡¹è¯„ä¼°
        results = {
            'forward_network_evaluation': self.evaluate_forward_network(num_samples),
            'pigan_evaluation': self.evaluate_pigan(num_samples),
            'structural_prediction_evaluation': self.evaluate_structural_prediction(min(num_samples//2, 500)),
            'model_validation': self.evaluate_model_validation(min(num_samples//2, 500)),
            'evaluation_time': time.time() - start_time,
            'total_samples': num_samples
        }
        
        # ä¿å­˜ç»“æœ
        self.evaluation_results = results
        
        # ç”Ÿæˆå¯è§†åŒ–
        print(f"\nğŸ¨ Generating evaluation visualizations...")
        self.generate_visualizations(results)
        
        print(f"\n" + "="*80)
        print(f"EVALUATION COMPLETED in {results['evaluation_time']:.2f}s")
        print("="*80)
        
        return results
    
    def generate_visualizations(self, results: Dict[str, Any]) -> None:
        """
        ç”Ÿæˆæ‰€æœ‰è¯„ä¼°ç»“æœçš„å¯è§†åŒ–
        
        Args:
            results: å®Œæ•´è¯„ä¼°ç»“æœ
        """
        try:
            # 1. å‰å‘ç½‘ç»œè¯„ä¼°å¯è§†åŒ–
            fwd_data = results['forward_network_evaluation'].get('data_samples', {})
            fwd_plot_path = self.visualizer.plot_forward_network_evaluation(
                results['forward_network_evaluation'], 
                fwd_data
            )
            print(f"âœ“ Forward network evaluation plot saved: {fwd_plot_path}")
            
            # 2. PI-GANè¯„ä¼°å¯è§†åŒ–
            pigan_data = results['pigan_evaluation'].get('data_samples', {})
            score_data = results['pigan_evaluation'].get('score_distributions', {})
            pigan_plot_path = self.visualizer.plot_pigan_evaluation(
                results['pigan_evaluation'],
                pigan_data,
                score_data
            )
            print(f"âœ“ PI-GAN evaluation plot saved: {pigan_plot_path}")
            
            # 3. ç»“æ„é¢„æµ‹è¯„ä¼°å¯è§†åŒ–
            struct_plot_path = self.visualizer.plot_structural_prediction_evaluation(
                results['structural_prediction_evaluation']
            )
            print(f"âœ“ Structural prediction evaluation plot saved: {struct_plot_path}")
            
            # 4. æ¨¡å‹éªŒè¯è¯„ä¼°å¯è§†åŒ–
            validation_plot_path = self.visualizer.plot_model_validation_evaluation(
                results['model_validation']
            )
            print(f"âœ“ Model validation evaluation plot saved: {validation_plot_path}")
            
            # 5. ç»¼åˆæ‘˜è¦å¯è§†åŒ–
            summary_plot_path = self.visualizer.plot_comprehensive_summary(results)
            print(f"âœ“ Comprehensive summary plot saved: {summary_plot_path}")
            
            print(f"ğŸ¯ All evaluation visualizations generated in: {self.visualizer.save_dir}")
            
        except Exception as e:
            print(f"âš  Warning: Failed to generate some visualizations: {e}")
    
    def generate_summary_report(self, save_path: str = None) -> str:
        """
        ç”Ÿæˆè¯„ä¼°æ€»ç»“æŠ¥å‘Š
        
        Args:
            save_path: æŠ¥å‘Šä¿å­˜è·¯å¾„
            
        Returns:
            str: æŠ¥å‘Šå†…å®¹
        """
        if not self.evaluation_results:
            raise ValueError("No evaluation results available. Run comprehensive evaluation first.")
        
        report_lines = []
        report_lines.append("="*80)
        report_lines.append("PI-GAN UNIFIED EVALUATION REPORT")
        report_lines.append("="*80)
        
        # åŸºæœ¬ä¿¡æ¯
        report_lines.append(f"Evaluation Date: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"Total Samples: {self.evaluation_results['total_samples']}")
        report_lines.append(f"Evaluation Time: {self.evaluation_results['evaluation_time']:.2f}s")
        report_lines.append("")
        
        # 1. å‰å‘ç½‘ç»œè¯„ä¼°
        fwd_results = self.evaluation_results['forward_network_evaluation']
        report_lines.append("1. FORWARD NETWORK EVALUATION")
        report_lines.append("-" * 40)
        spectrum_r2 = fwd_results['spectrum_prediction']['r2']
        metrics_r2 = fwd_results['metrics_prediction']['r2']
        report_lines.append(f"Spectrum Prediction RÂ²: {spectrum_r2:.4f}")
        report_lines.append(f"Metrics Prediction RÂ²: {metrics_r2:.4f}")
        if spectrum_r2 > 0.9 and metrics_r2 > 0.9:
            report_lines.append("âœ“ Forward network shows EXCELLENT performance")
        elif spectrum_r2 > 0.8 and metrics_r2 > 0.8:
            report_lines.append("âœ“ Forward network shows GOOD performance")
        else:
            report_lines.append("âš  Forward network needs improvement")
        report_lines.append("")
        
        # 2. PI-GANè¯„ä¼°
        pigan_results = self.evaluation_results['pigan_evaluation']
        report_lines.append("2. PI-GAN EVALUATION")
        report_lines.append("-" * 40)
        param_r2 = pigan_results['parameter_prediction']['r2']
        disc_acc = pigan_results['discriminator_performance']['overall_accuracy']
        report_lines.append(f"Parameter Prediction RÂ²: {param_r2:.4f}")
        report_lines.append(f"Discriminator Accuracy: {disc_acc:.4f}")
        if param_r2 > 0.8 and disc_acc > 0.8:
            report_lines.append("âœ“ PI-GAN shows EXCELLENT performance")
        elif param_r2 > 0.6 and disc_acc > 0.7:
            report_lines.append("âœ“ PI-GAN shows GOOD performance")
        else:
            report_lines.append("âš  PI-GAN needs improvement")
        report_lines.append("")
        
        # 3. ç»“æ„é¢„æµ‹è¯„ä¼°
        struct_results = self.evaluation_results['structural_prediction_evaluation']
        report_lines.append("3. STRUCTURAL PREDICTION EVALUATION")
        report_lines.append("-" * 40)
        violation_rate = struct_results['param_range_violation_rate']
        consistency = struct_results['consistency_score_mean']
        report_lines.append(f"Parameter Violation Rate: {violation_rate:.4f}")
        report_lines.append(f"Consistency Score: {consistency:.4f}")
        if violation_rate < 0.1 and consistency > 0.8:
            report_lines.append("âœ“ Structural prediction is RELIABLE")
        elif violation_rate < 0.2 and consistency > 0.6:
            report_lines.append("âœ“ Structural prediction is ACCEPTABLE")
        else:
            report_lines.append("âš  Structural prediction needs improvement")
        report_lines.append("")
        
        # 4. æ¨¡å‹éªŒè¯
        valid_results = self.evaluation_results['model_validation']
        report_lines.append("4. MODEL VALIDATION")
        report_lines.append("-" * 40)
        cycle_error = valid_results['cycle_consistency_error_mean']
        stability = valid_results['prediction_stability_mean']
        plausibility = valid_results['physical_plausibility_mean']
        report_lines.append(f"Cycle Consistency Error: {cycle_error:.6f}")
        report_lines.append(f"Prediction Stability: {stability:.6f}")
        report_lines.append(f"Physical Plausibility: {plausibility:.4f}")
        if cycle_error < 0.01 and stability < 0.01 and plausibility > 0.8:
            report_lines.append("âœ“ Model validation is EXCELLENT")
        elif cycle_error < 0.05 and stability < 0.05 and plausibility > 0.6:
            report_lines.append("âœ“ Model validation is GOOD")
        else:
            report_lines.append("âš  Model validation shows concerns")
        report_lines.append("")
        
        # æ€»ç»“
        report_lines.append("5. OVERALL ASSESSMENT")
        report_lines.append("-" * 40)
        excellent_count = sum([
            spectrum_r2 > 0.9 and metrics_r2 > 0.9,
            param_r2 > 0.8 and disc_acc > 0.8,
            violation_rate < 0.1 and consistency > 0.8,
            cycle_error < 0.01 and stability < 0.01 and plausibility > 0.8
        ])
        
        if excellent_count >= 3:
            report_lines.append("ğŸ¯ OVERALL RATING: EXCELLENT")
        elif excellent_count >= 2:
            report_lines.append("âœ… OVERALL RATING: GOOD")
        else:
            report_lines.append("âš ï¸ OVERALL RATING: NEEDS IMPROVEMENT")
        
        report_lines.append("="*80)
        
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        if save_path is None:
            save_path = os.path.join(self.visualizer.save_dir, "unified_evaluation_report.txt")
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\nEvaluation report saved to: {save_path}")
        return report_content

# ä¸»å‡½æ•°
def main():
    parser = argparse.ArgumentParser(description="Run unified PI-GAN evaluation")
    parser.add_argument('--model_dir', type=str, default=None,
                        help='Directory containing trained models')
    parser.add_argument('--data_path', type=str, default=None,
                        help='Path to dataset CSV file')
    parser.add_argument('--num_samples', type=int, default=1000,
                        help='Number of samples for evaluation')
    parser.add_argument('--device', type=str, default='auto',
                        help='Device to use (auto, cpu, cuda)')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed for reproducibility')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = UnifiedEvaluator(device=args.device)
    
    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    if not evaluator.load_models(args.model_dir):
        print("Failed to load models!")
        return
    
    if not evaluator.load_dataset(args.data_path):
        print("Failed to load dataset!")
        return
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.run_comprehensive_evaluation(args.num_samples)
    
    # ç”ŸæˆæŠ¥å‘Š
    evaluator.generate_summary_report()
    
    print("\nâœ… Unified evaluation completed successfully!")

if __name__ == "__main__":
    main()