#!/usr/bin/env python3
"""
æµ‹è¯•ç»Ÿä¸€è®­ç»ƒç³»ç»Ÿ
"""

import os
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.abspath(os.path.dirname(__file__))
sys.path.insert(0, project_root)

def test_unified_trainer_structure():
    """æµ‹è¯•ç»Ÿä¸€è®­ç»ƒå™¨ç»“æ„"""
    print("=== æµ‹è¯•ç»Ÿä¸€è®­ç»ƒå™¨ç»“æ„ ===")
    
    # æ£€æŸ¥ç»Ÿä¸€è®­ç»ƒå™¨æ–‡ä»¶
    unified_trainer_path = os.path.join(project_root, "core/train/unified_trainer.py")
    if os.path.exists(unified_trainer_path):
        print("âœ“ unified_trainer.py å­˜åœ¨")
    else:
        print("âœ— unified_trainer.py ç¼ºå¤±")
        return False
    
    # æ£€æŸ¥æ–‡ä»¶å†…å®¹
    with open(unified_trainer_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    required_classes = [
        'class UnifiedTrainer',
    ]
    
    required_methods = [
        'def train_forward_model_only',
        'def train_pigan_only', 
        'def train_full_pipeline',
        'def save_final_models',
        'def plot_training_curves'
    ]
    
    for cls in required_classes:
        if cls in content:
            print(f"âœ“ {cls} å­˜åœ¨")
        else:
            print(f"âœ— {cls} ç¼ºå¤±")
            return False
    
    for method in required_methods:
        if method in content:
            print(f"âœ“ {method} å­˜åœ¨")
        else:
            print(f"âœ— {method} ç¼ºå¤±")
            return False
    
    return True

def test_model_save_paths():
    """æµ‹è¯•æ¨¡å‹ä¿å­˜è·¯å¾„"""
    print("\n=== æµ‹è¯•æ¨¡å‹ä¿å­˜è·¯å¾„ ===")
    
    # æ£€æŸ¥ä¿å­˜ç›®å½•é…ç½®
    try:
        import config.config as cfg
        print(f"âœ“ SAVED_MODELS_DIR: {cfg.SAVED_MODELS_DIR}")
        print(f"âœ“ CHECKPOINT_DIR: {cfg.CHECKPOINT_DIR}")
        print(f"âœ“ PROJECT_ROOT: {cfg.PROJECT_ROOT}")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(cfg.SAVED_MODELS_DIR, exist_ok=True)
        os.makedirs(cfg.CHECKPOINT_DIR, exist_ok=True)
        os.makedirs(os.path.join(cfg.PROJECT_ROOT, "plots"), exist_ok=True)
        
        print("âœ“ æ‰€æœ‰å¿…éœ€ç›®å½•å·²åˆ›å»º")
        return True
        
    except Exception as e:
        print(f"âœ— é…ç½®é”™è¯¯: {e}")
        return False

def test_training_modes():
    """æµ‹è¯•è®­ç»ƒæ¨¡å¼å‚æ•°"""
    print("\n=== æµ‹è¯•è®­ç»ƒæ¨¡å¼ ===")
    
    unified_trainer_path = os.path.join(project_root, "core/train/unified_trainer.py")
    with open(unified_trainer_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥è®­ç»ƒæ¨¡å¼æ”¯æŒ
    modes = ['forward_only', 'pigan_only', 'full']
    for mode in modes:
        if f"'{mode}'" in content:
            print(f"âœ“ æ”¯æŒè®­ç»ƒæ¨¡å¼: {mode}")
        else:
            print(f"âœ— ç¼ºå¤±è®­ç»ƒæ¨¡å¼: {mode}")
            return False
    
    # æ£€æŸ¥æ¨¡å‹ä¿å­˜æ–‡ä»¶å
    expected_files = ['generator_final.pth', 'discriminator_final.pth', 'forward_model_final.pth']
    for filename in expected_files:
        if filename in content:
            print(f"âœ“ ä¿å­˜æ–‡ä»¶åæ­£ç¡®: {filename}")
        else:
            print(f"âœ— æ–‡ä»¶åç¼ºå¤±: {filename}")
            return False
    
    return True

def test_evaluation_compatibility():
    """æµ‹è¯•ä¸è¯„ä¼°å™¨çš„å…¼å®¹æ€§"""
    print("\n=== æµ‹è¯•è¯„ä¼°å™¨å…¼å®¹æ€§ ===")
    
    # æ£€æŸ¥è¯„ä¼°å™¨æœŸæœ›çš„æ–‡ä»¶å
    unified_evaluator_path = os.path.join(project_root, "core/evaluate/unified_evaluator.py")
    if not os.path.exists(unified_evaluator_path):
        print("âœ— unified_evaluator.py ä¸å­˜åœ¨")
        return False
    
    with open(unified_evaluator_path, 'r', encoding='utf-8') as f:
        eval_content = f.read()
    
    # æ£€æŸ¥è¯„ä¼°å™¨æœŸæœ›çš„æ–‡ä»¶å
    expected_model_files = [
        'generator_final.pth',
        'discriminator_final.pth', 
        'forward_model_final.pth'
    ]
    
    for filename in expected_model_files:
        if filename in eval_content:
            print(f"âœ“ è¯„ä¼°å™¨æœŸæœ›æ–‡ä»¶: {filename}")
        else:
            print(f"âœ— è¯„ä¼°å™¨æœªæ‰¾åˆ°æœŸæœ›æ–‡ä»¶: {filename}")
            return False
    
    return True

def test_import_structure():
    """æµ‹è¯•å¯¼å…¥ç»“æ„"""
    print("\n=== æµ‹è¯•å¯¼å…¥ç»“æ„ ===")
    
    try:
        # æµ‹è¯•é…ç½®å¯¼å…¥
        import config.config as cfg
        print("âœ“ config.config å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•ä¼˜åŒ–é…ç½®å¯¼å…¥
        from config.training_optimization import get_optimization_config
        opt_config = get_optimization_config()
        print("âœ“ training_optimization å¯¼å…¥æˆåŠŸ")
        print(f"  - æŸå¤±æƒé‡é…ç½®: {len(opt_config['loss_weights'])} é¡¹")
        print(f"  - æ¨¡å‹æ¶æ„é…ç½®: {len(opt_config['model_architecture'])} é¡¹")
        
        return True
        
    except ImportError as e:
        print(f"âœ— å¯¼å…¥é”™è¯¯: {e}")
        return False

def create_mock_run_script():
    """åˆ›å»ºæ¨¡æ‹Ÿè¿è¡Œè„šæœ¬"""
    print("\n=== åˆ›å»ºæµ‹è¯•è„šæœ¬ ===")
    
    test_script_content = '''#!/usr/bin/env python3
"""
æ¨¡æ‹Ÿè®­ç»ƒè„šæœ¬ - ç”¨äºå¿«é€Ÿæµ‹è¯•
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.abspath(os.path.dirname(__file__))
sys.path.insert(0, project_root)

def mock_training_test():
    """æ¨¡æ‹Ÿè®­ç»ƒæµ‹è¯•ï¼ˆä¸å®é™…è®­ç»ƒï¼‰"""
    print("ğŸ§ª æ¨¡æ‹Ÿè®­ç»ƒæµ‹è¯•")
    
    try:
        # æµ‹è¯•å¯¼å…¥
        from core.train.unified_trainer import UnifiedTrainer
        print("âœ“ UnifiedTrainer å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•åˆå§‹åŒ–
        trainer = UnifiedTrainer(device="cpu")
        print("âœ“ UnifiedTrainer åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
        trainer.initialize_models()
        print("âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹ä¿å­˜ï¼ˆåˆ›å»ºç©ºæ–‡ä»¶è¿›è¡Œæµ‹è¯•ï¼‰
        import config.config as cfg
        os.makedirs(cfg.SAVED_MODELS_DIR, exist_ok=True)
        
        # æ¨¡æ‹Ÿä¿å­˜æ¨¡å‹æ–‡ä»¶
        import torch
        dummy_state = {'test': torch.tensor([1.0])}
        
        model_files = [
            "generator_final.pth",
            "discriminator_final.pth", 
            "forward_model_final.pth"
        ]
        
        for filename in model_files:
            filepath = os.path.join(cfg.SAVED_MODELS_DIR, filename)
            torch.save(dummy_state, filepath)
            print(f"âœ“ æ¨¡æ‹Ÿä¿å­˜: {filename}")
        
        print("ğŸ‰ æ¨¡æ‹Ÿè®­ç»ƒæµ‹è¯•é€šè¿‡ï¼")
        print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨: {cfg.SAVED_MODELS_DIR}")
        
        # æµ‹è¯•è¯„ä¼°å™¨å…¼å®¹æ€§
        print("\\nğŸ” æµ‹è¯•è¯„ä¼°å™¨å…¼å®¹æ€§...")
        from core.evaluate.unified_evaluator import UnifiedEvaluator
        evaluator = UnifiedEvaluator(device="cpu")
        print("âœ“ UnifiedEvaluator åˆå§‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦èƒ½è¢«æ‰¾åˆ°
        for filename in model_files:
            filepath = os.path.join(cfg.SAVED_MODELS_DIR, filename)
            if os.path.exists(filepath):
                print(f"âœ“ æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {filename}")
            else:
                print(f"âœ— æ¨¡å‹æ–‡ä»¶ç¼ºå¤±: {filename}")
        
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç»Ÿä¸€è®­ç»ƒç³»ç»Ÿå·²å°±ç»ªã€‚")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    mock_training_test()
'''
    
    script_path = os.path.join(project_root, "mock_training_test.py")
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(test_script_content)
    
    print(f"âœ“ æµ‹è¯•è„šæœ¬å·²åˆ›å»º: {script_path}")
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("PI-GAN ç»Ÿä¸€è®­ç»ƒç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    
    all_tests_passed = True
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        test_unified_trainer_structure,
        test_model_save_paths,
        test_training_modes,
        test_evaluation_compatibility,
        test_import_structure,
        create_mock_run_script
    ]
    
    for test in tests:
        if not test():
            all_tests_passed = False
    
    print("\n" + "=" * 60)
    if all_tests_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç»Ÿä¸€è®­ç»ƒç³»ç»Ÿå·²å°±ç»ª")
        print("\nğŸ“‹ ç³»ç»Ÿç‰¹æ€§ï¼š")
        print("  âœ“ ç»Ÿä¸€è®­ç»ƒå™¨æ•´åˆä¸‰ç§è®­ç»ƒæ¨¡å¼")
        print("  âœ“ ä¿®å¤æ¨¡å‹ä¿å­˜è·¯å¾„é—®é¢˜")
        print("  âœ“ ä¸è¯„ä¼°å™¨å®Œå…¨å…¼å®¹")
        print("  âœ“ å®Œæ•´çš„è®­ç»ƒæµæ°´çº¿")
        print("  âœ“ å®æ—¶è®­ç»ƒç›‘æ§å’Œå¯è§†åŒ–")
        print("\nğŸš€ å¿«é€Ÿå¼€å§‹ï¼š")
        print("  python core/train/unified_trainer.py --mode full")
        print("  python core/evaluate/unified_evaluator.py --num_samples 1000")
        print("\nğŸ§ª æ¨¡æ‹Ÿæµ‹è¯•ï¼š")
        print("  python mock_training_test.py")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³æ–‡ä»¶")
    
    print("=" * 60)
    return all_tests_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)